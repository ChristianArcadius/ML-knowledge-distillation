{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-DyWm3_Kme6"
   },
   "source": [
    "# VGG-16 Pytorch Implementation\n",
    "### Code written following this tutorial blog: https://blog.paperspace.com/vgg-from-scratch-pytorch/\n",
    "### VGG paper https://arxiv.org/pdf/1409.1556.pdf?ref=blog.paperspace.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 13250,
     "status": "ok",
     "timestamp": 1713804190520,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "PvZXBp_pHSPo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# DirectML for AMD compatibility\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713804190520,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "sbkLQsLbHyOF",
    "outputId": "6e1556a8-490f-4f44-dad8-a43658ce1cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch_directml.device(torch_directml.default_device())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcUfvwvGOREX"
   },
   "source": [
    "## Data Loaders\n",
    "### Some notes on this data:\n",
    "  - These data loaders create 228x228 images whereas the original VGG paper states that the images inputted to the model were 224x224\n",
    "  - The images from the CIFAR-10 dataset have only a 32x32 resolution. The original VGG model was trained with the ILSVRC dataset which had an average resolution of 469x387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713804190520,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "6f2jqwIUHzRk"
   },
   "outputs": [],
   "source": [
    "# Create a data loader for handling the CIFAR-10 dataset\n",
    "  # this version of CIFAR  60,000 images with 10 different animal classes\n",
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "\n",
    "    # mean and std of the rgb values in the images\n",
    "    normalize = transforms.Normalize( # tutorial claimed these values available online\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((227, 227)), # in vgg paper, images are 224x224, not sure why they chose 227 (228x228)\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    ### Test Data ###\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    ### Training and Validation Data ###\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34619,
     "status": "ok",
     "timestamp": 1713804225137,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "VYa4BRPkLv8s",
    "outputId": "6d46616d-9b5c-4348-e040-045c4274746a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=32)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=32,\n",
    "                              test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY5BichlOWPS"
   },
   "source": [
    "## VGG-16\n",
    "\n",
    "Some notes on this implementation:\n",
    "  - After each convolution, nn.BatchNorm2d() is used to normalize all the convolution channel outputs. However, the original paper explicitly states that local normalization does not improve performance.\n",
    "  - The forward function uses out.reshape(out.size(0), -1) to change the data outputted by the convolution layers before they are sent to the linear/fully connected layers. However, there is no mention of any data transformation between these layers in the original VGG paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1713804225137,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "Qgza0vPfOZX-"
   },
   "outputs": [],
   "source": [
    "# Design the model's neural network\n",
    "class VGG16(nn.Module): # all torch nn models must subclass nn.Module\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        # Convolution layer: 3 input channels (rgb), 64 output channels, 3x3 kernel\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64), # normalize the convolution output\n",
    "            nn.ReLU())  # activation function essentially throws out values < 0\n",
    "\n",
    "        # Convolution layer: 64 input, 64 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 64 input, 128 output, 3x3 kernel\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 128 input, 128 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 128 input, 256 output, 3x3 kernel\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 256 input, 256 output, 3x3 kernel\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 256 input, 256 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 256 input, 512 output, 3x3 kernel\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7 * 7 * 512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # This is the layer that performs the classification\n",
    "          # it takes the 4096 input channels from fc1 and outputs probabilities of each class in CIFAR\n",
    "          # the outputs (num_classes) depend on if we classify super or fine classes in CIFAR-100 (10 or 100 classes)\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "\n",
    "    # Define how the layers are connected\n",
    "    def forward(self, x):\n",
    "      out = self.layer1(x)\n",
    "      out = self.layer2(out)\n",
    "      out = self.layer3(out)\n",
    "      out = self.layer4(out)\n",
    "      out = self.layer5(out)\n",
    "      out = self.layer6(out)\n",
    "      out = self.layer7(out)\n",
    "      out = self.layer8(out)\n",
    "      out = self.layer9(out)\n",
    "      out = self.layer10(out)\n",
    "      out = self.layer11(out)\n",
    "      out = self.layer12(out)\n",
    "      out = self.layer13(out)\n",
    "      out = out.reshape(out.size(0), -1) # Not fully sure what this is doing or if it is true to the original VGG\n",
    "      out = self.fc(out)\n",
    "      out = self.fc1(out)\n",
    "      out = self.fc2(out)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daf39ubCa-HB"
   },
   "source": [
    "## Training the VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1713804227798,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "Ur3OldsKaYHV"
   },
   "outputs": [],
   "source": [
    "### Hyper Parameters ###\n",
    "\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = VGG16(num_classes).to(device) # use GPU\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in this implementation of VGG-16: 65103946\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters in this implementation of VGG-16: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEgAnNULbH_Z",
    "outputId": "b0ba2ee7-988f-4e3f-95e2-c7affaab801e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/1407], Loss: 2.2793\n",
      "Epoch [1/10], Step [2/1407], Loss: 2.3592\n",
      "Epoch [1/10], Step [3/1407], Loss: 2.3043\n",
      "Epoch [1/10], Step [4/1407], Loss: 2.3223\n",
      "Epoch [1/10], Step [5/1407], Loss: 2.2440\n",
      "Epoch [1/10], Step [6/1407], Loss: 2.4693\n",
      "Epoch [1/10], Step [7/1407], Loss: 2.4158\n",
      "Epoch [1/10], Step [8/1407], Loss: 2.3405\n",
      "Epoch [1/10], Step [9/1407], Loss: 2.3697\n",
      "Epoch [1/10], Step [10/1407], Loss: 2.4740\n",
      "Epoch [1/10], Step [11/1407], Loss: 2.3066\n",
      "Epoch [1/10], Step [12/1407], Loss: 2.3142\n",
      "Epoch [1/10], Step [13/1407], Loss: 2.4122\n",
      "Epoch [1/10], Step [14/1407], Loss: 2.1970\n",
      "Epoch [1/10], Step [15/1407], Loss: 2.3630\n",
      "Epoch [1/10], Step [16/1407], Loss: 2.2087\n",
      "Epoch [1/10], Step [17/1407], Loss: 2.2916\n",
      "Epoch [1/10], Step [18/1407], Loss: 2.3902\n",
      "Epoch [1/10], Step [19/1407], Loss: 2.2089\n",
      "Epoch [1/10], Step [20/1407], Loss: 2.4710\n",
      "Epoch [1/10], Step [21/1407], Loss: 2.1822\n",
      "Epoch [1/10], Step [22/1407], Loss: 2.3189\n",
      "Epoch [1/10], Step [23/1407], Loss: 2.1431\n",
      "Epoch [1/10], Step [24/1407], Loss: 2.1210\n",
      "Epoch [1/10], Step [25/1407], Loss: 2.3298\n",
      "Epoch [1/10], Step [26/1407], Loss: 2.4229\n",
      "Epoch [1/10], Step [27/1407], Loss: 2.4527\n",
      "Epoch [1/10], Step [28/1407], Loss: 2.3837\n",
      "Epoch [1/10], Step [29/1407], Loss: 2.4575\n",
      "Epoch [1/10], Step [30/1407], Loss: 2.2755\n",
      "Epoch [1/10], Step [31/1407], Loss: 2.3009\n",
      "Epoch [1/10], Step [32/1407], Loss: 2.2682\n",
      "Epoch [1/10], Step [33/1407], Loss: 2.2602\n",
      "Epoch [1/10], Step [34/1407], Loss: 2.2042\n",
      "Epoch [1/10], Step [35/1407], Loss: 2.3064\n",
      "Epoch [1/10], Step [36/1407], Loss: 2.3132\n",
      "Epoch [1/10], Step [37/1407], Loss: 2.3882\n",
      "Epoch [1/10], Step [38/1407], Loss: 2.2089\n",
      "Epoch [1/10], Step [39/1407], Loss: 2.1646\n",
      "Epoch [1/10], Step [40/1407], Loss: 2.2016\n",
      "Epoch [1/10], Step [41/1407], Loss: 2.1811\n",
      "Epoch [1/10], Step [42/1407], Loss: 2.0552\n",
      "Epoch [1/10], Step [43/1407], Loss: 2.0675\n",
      "Epoch [1/10], Step [44/1407], Loss: 2.3002\n",
      "Epoch [1/10], Step [45/1407], Loss: 2.2287\n",
      "Epoch [1/10], Step [46/1407], Loss: 2.1272\n",
      "Epoch [1/10], Step [47/1407], Loss: 2.2481\n",
      "Epoch [1/10], Step [48/1407], Loss: 2.0924\n",
      "Epoch [1/10], Step [49/1407], Loss: 2.2975\n",
      "Epoch [1/10], Step [50/1407], Loss: 2.4105\n",
      "Epoch [1/10], Step [51/1407], Loss: 2.0895\n",
      "Epoch [1/10], Step [52/1407], Loss: 2.1354\n",
      "Epoch [1/10], Step [53/1407], Loss: 2.2251\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[105], line 14\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\directML\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\anaconda3\\envs\\directML\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not allocate tensor with 949782528 bytes. There is not enough GPU video memory available!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        \n",
    "    # Validation\n",
    "      # a validation dataset allows us to see model progress along the way while saving our true test data for the end\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1itjIa4peSYk"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKkcfeOocZYa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81.16 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test dataset\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './VGG-16_CIFAR-10_228x228.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 with 128x128 images 10 epochs on CIFAR-10\n",
    "    ~82.61%\n",
    "### VGG-16 with 228x228 images 10 epochs on CIFAR-10\n",
    "    ~81.16%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNbT3YYoHjfEeTla+pwsK0l",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "directML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
