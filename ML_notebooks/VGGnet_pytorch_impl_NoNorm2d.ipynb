{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-DyWm3_Kme6"
   },
   "source": [
    "# VGG-16 Pytorch Implementation\n",
    "### Code written following this tutorial blog: https://blog.paperspace.com/vgg-from-scratch-pytorch/\n",
    "### VGG paper https://arxiv.org/pdf/1409.1556.pdf?ref=blog.paperspace.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13250,
     "status": "ok",
     "timestamp": 1713804190520,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "PvZXBp_pHSPo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# DirectML for AMD compatibility\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713804190520,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "sbkLQsLbHyOF",
    "outputId": "6e1556a8-490f-4f44-dad8-a43658ce1cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch_directml.device(torch_directml.default_device())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcUfvwvGOREX"
   },
   "source": [
    "## Data Loaders\n",
    "### Some notes on this data:\n",
    "  - These data loaders create 228x228 images whereas the original VGG paper states that the images inputted to the model were 224x224\n",
    "  - The images from the CIFAR-10 dataset have only a 32x32 resolution. The original VGG model was trained with the ILSVRC dataset which had an average resolution of 469x387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713804190520,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "6f2jqwIUHzRk"
   },
   "outputs": [],
   "source": [
    "# Create a data loader for handling the CIFAR-10 dataset\n",
    "  # this version of CIFAR  60,000 images with 10 different animal classes\n",
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "\n",
    "    # mean and std of the rgb values in the images\n",
    "    normalize = transforms.Normalize( # tutorial claimed these values available online\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)), # in vgg paper, images are 224x224, not sure why they chose 227 (228x228)\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    ### Test Data ###\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR100(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    ### Training and Validation Data ###\n",
    "    train_dataset = datasets.CIFAR100(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR100(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34619,
     "status": "ok",
     "timestamp": 1713804225137,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "VYa4BRPkLv8s",
    "outputId": "6d46616d-9b5c-4348-e040-045c4274746a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=32)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=32,\n",
    "                              test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY5BichlOWPS"
   },
   "source": [
    "## VGG-16\n",
    "\n",
    "Some notes on this implementation:\n",
    "  - After each convolution, nn.BatchNorm2d() is used to normalize all the convolution channel outputs. However, the original paper explicitly states that local normalization does not improve performance.\n",
    "  - The forward function uses out.reshape(out.size(0), -1) to change the data outputted by the convolution layers before they are sent to the linear/fully connected layers. However, there is no mention of any data transformation between these layers in the original VGG paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1713804225137,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "Qgza0vPfOZX-"
   },
   "outputs": [],
   "source": [
    "# Design the model's neural network\n",
    "class VGG16(nn.Module): # all torch nn models must subclass nn.Module\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        # Convolution layer: 3 input channels (rgb), 64 output channels, 3x3 kernel\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())  # activation function essentially throws out values < 0\n",
    "\n",
    "        # Convolution layer: 64 input, 64 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 64 input, 128 output, 3x3 kernel\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 128 input, 128 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 128 input, 256 output, 3x3 kernel\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 256 input, 256 output, 3x3 kernel\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 256 input, 256 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 256 input, 512 output, 3x3 kernel\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Convolution layer: 512 input, 512 output, 3x3 kernel\n",
    "        # Max pooling into a 2x2 kernel\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # This is the layer that performs the classification\n",
    "          # it takes the 4096 input channels from fc1 and outputs probabilities of each class in CIFAR\n",
    "          # the outputs (num_classes) depend on if we classify super or fine classes in CIFAR-100 (10 or 100 classes)\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "\n",
    "    # Define how the layers are connected\n",
    "    def forward(self, x):\n",
    "      out = self.layer1(x)\n",
    "      out = self.layer2(out)\n",
    "      out = self.layer3(out)\n",
    "      out = self.layer4(out)\n",
    "      out = self.layer5(out)\n",
    "      out = self.layer6(out)\n",
    "      out = self.layer7(out)\n",
    "      out = self.layer8(out)\n",
    "      out = self.layer9(out)\n",
    "      out = self.layer10(out)\n",
    "      out = self.layer11(out)\n",
    "      out = self.layer12(out)\n",
    "      out = self.layer13(out)\n",
    "      out = out.reshape(out.size(0), -1) # Not fully sure what this is doing or if it is true to the original VGG\n",
    "      out = self.fc(out)\n",
    "      out = self.fc1(out)\n",
    "      out = self.fc2(out)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daf39ubCa-HB"
   },
   "source": [
    "## Training the VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1713804227798,
     "user": {
      "displayName": "Christian Di Troia",
      "userId": "04358147416681017939"
     },
     "user_tz": 240
    },
    "id": "Ur3OldsKaYHV"
   },
   "outputs": [],
   "source": [
    "### Hyper Parameters ###\n",
    "\n",
    "num_classes = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.008\n",
    "\n",
    "model = VGG16(num_classes).to(device) # use GPU\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEgAnNULbH_Z",
    "outputId": "b0ba2ee7-988f-4e3f-95e2-c7affaab801e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [2/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [3/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [4/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [5/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [6/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [7/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [8/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [9/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [10/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [11/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [12/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [13/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [14/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [15/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [16/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [17/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [18/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [19/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [20/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [21/1407], Loss: 4.6014\n",
      "Epoch [1/2], Step [22/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [23/1407], Loss: 4.6043\n",
      "Epoch [1/2], Step [24/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [25/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [26/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [27/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [28/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [29/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [30/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [31/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [32/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [33/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [34/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [35/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [36/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [37/1407], Loss: 4.6005\n",
      "Epoch [1/2], Step [38/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [39/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [40/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [41/1407], Loss: 4.6079\n",
      "Epoch [1/2], Step [42/1407], Loss: 4.6013\n",
      "Epoch [1/2], Step [43/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [44/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [45/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [46/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [47/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [48/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [49/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [50/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [51/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [52/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [53/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [54/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [55/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [56/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [57/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [58/1407], Loss: 4.6005\n",
      "Epoch [1/2], Step [59/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [60/1407], Loss: 4.6019\n",
      "Epoch [1/2], Step [61/1407], Loss: 4.6034\n",
      "Epoch [1/2], Step [62/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [63/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [64/1407], Loss: 4.6142\n",
      "Epoch [1/2], Step [65/1407], Loss: 4.6000\n",
      "Epoch [1/2], Step [66/1407], Loss: 4.6024\n",
      "Epoch [1/2], Step [67/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [68/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [69/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [70/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [71/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [72/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [73/1407], Loss: 4.6024\n",
      "Epoch [1/2], Step [74/1407], Loss: 4.6032\n",
      "Epoch [1/2], Step [75/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [76/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [77/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [78/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [79/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [80/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [81/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [82/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [83/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [84/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [85/1407], Loss: 4.6024\n",
      "Epoch [1/2], Step [86/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [87/1407], Loss: 4.6119\n",
      "Epoch [1/2], Step [88/1407], Loss: 4.6007\n",
      "Epoch [1/2], Step [89/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [90/1407], Loss: 4.6003\n",
      "Epoch [1/2], Step [91/1407], Loss: 4.6003\n",
      "Epoch [1/2], Step [92/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [93/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [94/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [95/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [96/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [97/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [98/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [99/1407], Loss: 4.6012\n",
      "Epoch [1/2], Step [100/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [101/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [102/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [103/1407], Loss: 4.6096\n",
      "Epoch [1/2], Step [104/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [105/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [106/1407], Loss: 4.5997\n",
      "Epoch [1/2], Step [107/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [108/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [109/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [110/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [111/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [112/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [113/1407], Loss: 4.6032\n",
      "Epoch [1/2], Step [114/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [115/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [116/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [117/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [118/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [119/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [120/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [121/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [122/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [123/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [124/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [125/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [126/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [127/1407], Loss: 4.6020\n",
      "Epoch [1/2], Step [128/1407], Loss: 4.6105\n",
      "Epoch [1/2], Step [129/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [130/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [131/1407], Loss: 4.6109\n",
      "Epoch [1/2], Step [132/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [133/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [134/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [135/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [136/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [137/1407], Loss: 4.6093\n",
      "Epoch [1/2], Step [138/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [139/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [140/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [141/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [142/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [143/1407], Loss: 4.6135\n",
      "Epoch [1/2], Step [144/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [145/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [146/1407], Loss: 4.6121\n",
      "Epoch [1/2], Step [147/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [148/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [149/1407], Loss: 4.5983\n",
      "Epoch [1/2], Step [150/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [151/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [152/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [153/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [154/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [155/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [156/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [157/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [158/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [159/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [160/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [161/1407], Loss: 4.6129\n",
      "Epoch [1/2], Step [162/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [163/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [164/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [165/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [166/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [167/1407], Loss: 4.6014\n",
      "Epoch [1/2], Step [168/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [169/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [170/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [171/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [172/1407], Loss: 4.6105\n",
      "Epoch [1/2], Step [173/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [174/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [175/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [176/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [177/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [178/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [179/1407], Loss: 4.6019\n",
      "Epoch [1/2], Step [180/1407], Loss: 4.6121\n",
      "Epoch [1/2], Step [181/1407], Loss: 4.6003\n",
      "Epoch [1/2], Step [182/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [183/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [184/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [185/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [186/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [187/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [188/1407], Loss: 4.5974\n",
      "Epoch [1/2], Step [189/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [190/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [191/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [192/1407], Loss: 4.5981\n",
      "Epoch [1/2], Step [193/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [194/1407], Loss: 4.5995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [195/1407], Loss: 4.6093\n",
      "Epoch [1/2], Step [196/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [197/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [198/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [199/1407], Loss: 4.6151\n",
      "Epoch [1/2], Step [200/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [201/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [202/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [203/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [204/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [205/1407], Loss: 4.5989\n",
      "Epoch [1/2], Step [206/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [207/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [208/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [209/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [210/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [211/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [212/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [213/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [214/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [215/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [216/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [217/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [218/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [219/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [220/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [221/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [222/1407], Loss: 4.6012\n",
      "Epoch [1/2], Step [223/1407], Loss: 4.6112\n",
      "Epoch [1/2], Step [224/1407], Loss: 4.6014\n",
      "Epoch [1/2], Step [225/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [226/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [227/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [228/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [229/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [230/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [231/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [232/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [233/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [234/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [235/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [236/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [237/1407], Loss: 4.6019\n",
      "Epoch [1/2], Step [238/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [239/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [240/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [241/1407], Loss: 4.6034\n",
      "Epoch [1/2], Step [242/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [243/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [244/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [245/1407], Loss: 4.6021\n",
      "Epoch [1/2], Step [246/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [247/1407], Loss: 4.6132\n",
      "Epoch [1/2], Step [248/1407], Loss: 4.6160\n",
      "Epoch [1/2], Step [249/1407], Loss: 4.6131\n",
      "Epoch [1/2], Step [250/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [251/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [252/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [253/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [254/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [255/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [256/1407], Loss: 4.6018\n",
      "Epoch [1/2], Step [257/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [258/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [259/1407], Loss: 4.6019\n",
      "Epoch [1/2], Step [260/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [261/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [262/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [263/1407], Loss: 4.5987\n",
      "Epoch [1/2], Step [264/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [265/1407], Loss: 4.6001\n",
      "Epoch [1/2], Step [266/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [267/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [268/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [269/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [270/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [271/1407], Loss: 4.6157\n",
      "Epoch [1/2], Step [272/1407], Loss: 4.6127\n",
      "Epoch [1/2], Step [273/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [274/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [275/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [276/1407], Loss: 4.5981\n",
      "Epoch [1/2], Step [277/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [278/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [279/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [280/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [281/1407], Loss: 4.6109\n",
      "Epoch [1/2], Step [282/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [283/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [284/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [285/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [286/1407], Loss: 4.6136\n",
      "Epoch [1/2], Step [287/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [288/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [289/1407], Loss: 4.5997\n",
      "Epoch [1/2], Step [290/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [291/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [292/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [293/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [294/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [295/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [296/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [297/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [298/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [299/1407], Loss: 4.6132\n",
      "Epoch [1/2], Step [300/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [301/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [302/1407], Loss: 4.6131\n",
      "Epoch [1/2], Step [303/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [304/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [305/1407], Loss: 4.6111\n",
      "Epoch [1/2], Step [306/1407], Loss: 4.6121\n",
      "Epoch [1/2], Step [307/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [308/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [309/1407], Loss: 4.5997\n",
      "Epoch [1/2], Step [310/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [311/1407], Loss: 4.6109\n",
      "Epoch [1/2], Step [312/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [313/1407], Loss: 4.6011\n",
      "Epoch [1/2], Step [314/1407], Loss: 4.6011\n",
      "Epoch [1/2], Step [315/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [316/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [317/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [318/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [319/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [320/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [321/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [322/1407], Loss: 4.6024\n",
      "Epoch [1/2], Step [323/1407], Loss: 4.5956\n",
      "Epoch [1/2], Step [324/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [325/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [326/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [327/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [328/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [329/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [330/1407], Loss: 4.6011\n",
      "Epoch [1/2], Step [331/1407], Loss: 4.6014\n",
      "Epoch [1/2], Step [332/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [333/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [334/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [335/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [336/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [337/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [338/1407], Loss: 4.6021\n",
      "Epoch [1/2], Step [339/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [340/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [341/1407], Loss: 4.6002\n",
      "Epoch [1/2], Step [342/1407], Loss: 4.5995\n",
      "Epoch [1/2], Step [343/1407], Loss: 4.6171\n",
      "Epoch [1/2], Step [344/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [345/1407], Loss: 4.6020\n",
      "Epoch [1/2], Step [346/1407], Loss: 4.6110\n",
      "Epoch [1/2], Step [347/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [348/1407], Loss: 4.6113\n",
      "Epoch [1/2], Step [349/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [350/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [351/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [352/1407], Loss: 4.6034\n",
      "Epoch [1/2], Step [353/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [354/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [355/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [356/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [357/1407], Loss: 4.6102\n",
      "Epoch [1/2], Step [358/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [359/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [360/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [361/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [362/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [363/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [364/1407], Loss: 4.6043\n",
      "Epoch [1/2], Step [365/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [366/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [367/1407], Loss: 4.6011\n",
      "Epoch [1/2], Step [368/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [369/1407], Loss: 4.6136\n",
      "Epoch [1/2], Step [370/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [371/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [372/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [373/1407], Loss: 4.6100\n",
      "Epoch [1/2], Step [374/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [375/1407], Loss: 4.6102\n",
      "Epoch [1/2], Step [376/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [377/1407], Loss: 4.5994\n",
      "Epoch [1/2], Step [378/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [379/1407], Loss: 4.6105\n",
      "Epoch [1/2], Step [380/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [381/1407], Loss: 4.6112\n",
      "Epoch [1/2], Step [382/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [383/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [384/1407], Loss: 4.5970\n",
      "Epoch [1/2], Step [385/1407], Loss: 4.6089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [386/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [387/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [388/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [389/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [390/1407], Loss: 4.6119\n",
      "Epoch [1/2], Step [391/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [392/1407], Loss: 4.6129\n",
      "Epoch [1/2], Step [393/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [394/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [395/1407], Loss: 4.6002\n",
      "Epoch [1/2], Step [396/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [397/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [398/1407], Loss: 4.5981\n",
      "Epoch [1/2], Step [399/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [400/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [401/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [402/1407], Loss: 4.6000\n",
      "Epoch [1/2], Step [403/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [404/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [405/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [406/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [407/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [408/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [409/1407], Loss: 4.6124\n",
      "Epoch [1/2], Step [410/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [411/1407], Loss: 4.5999\n",
      "Epoch [1/2], Step [412/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [413/1407], Loss: 4.6151\n",
      "Epoch [1/2], Step [414/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [415/1407], Loss: 4.5985\n",
      "Epoch [1/2], Step [416/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [417/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [418/1407], Loss: 4.6126\n",
      "Epoch [1/2], Step [419/1407], Loss: 4.5991\n",
      "Epoch [1/2], Step [420/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [421/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [422/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [423/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [424/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [425/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [426/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [427/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [428/1407], Loss: 4.6146\n",
      "Epoch [1/2], Step [429/1407], Loss: 4.6119\n",
      "Epoch [1/2], Step [430/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [431/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [432/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [433/1407], Loss: 4.5984\n",
      "Epoch [1/2], Step [434/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [435/1407], Loss: 4.6011\n",
      "Epoch [1/2], Step [436/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [437/1407], Loss: 4.6079\n",
      "Epoch [1/2], Step [438/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [439/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [440/1407], Loss: 4.6100\n",
      "Epoch [1/2], Step [441/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [442/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [443/1407], Loss: 4.6096\n",
      "Epoch [1/2], Step [444/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [445/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [446/1407], Loss: 4.6011\n",
      "Epoch [1/2], Step [447/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [448/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [449/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [450/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [451/1407], Loss: 4.6120\n",
      "Epoch [1/2], Step [452/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [453/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [454/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [455/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [456/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [457/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [458/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [459/1407], Loss: 4.6107\n",
      "Epoch [1/2], Step [460/1407], Loss: 4.6162\n",
      "Epoch [1/2], Step [461/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [462/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [463/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [464/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [465/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [466/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [467/1407], Loss: 4.6010\n",
      "Epoch [1/2], Step [468/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [469/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [470/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [471/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [472/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [473/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [474/1407], Loss: 4.6107\n",
      "Epoch [1/2], Step [475/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [476/1407], Loss: 4.6131\n",
      "Epoch [1/2], Step [477/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [478/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [479/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [480/1407], Loss: 4.6122\n",
      "Epoch [1/2], Step [481/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [482/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [483/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [484/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [485/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [486/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [487/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [488/1407], Loss: 4.6130\n",
      "Epoch [1/2], Step [489/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [490/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [491/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [492/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [493/1407], Loss: 4.6006\n",
      "Epoch [1/2], Step [494/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [495/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [496/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [497/1407], Loss: 4.5970\n",
      "Epoch [1/2], Step [498/1407], Loss: 4.6122\n",
      "Epoch [1/2], Step [499/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [500/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [501/1407], Loss: 4.6141\n",
      "Epoch [1/2], Step [502/1407], Loss: 4.6032\n",
      "Epoch [1/2], Step [503/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [504/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [505/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [506/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [507/1407], Loss: 4.6127\n",
      "Epoch [1/2], Step [508/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [509/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [510/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [511/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [512/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [513/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [514/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [515/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [516/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [517/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [518/1407], Loss: 4.6133\n",
      "Epoch [1/2], Step [519/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [520/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [521/1407], Loss: 4.6043\n",
      "Epoch [1/2], Step [522/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [523/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [524/1407], Loss: 4.6006\n",
      "Epoch [1/2], Step [525/1407], Loss: 4.6009\n",
      "Epoch [1/2], Step [526/1407], Loss: 4.6114\n",
      "Epoch [1/2], Step [527/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [528/1407], Loss: 4.6141\n",
      "Epoch [1/2], Step [529/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [530/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [531/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [532/1407], Loss: 4.6096\n",
      "Epoch [1/2], Step [533/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [534/1407], Loss: 4.6021\n",
      "Epoch [1/2], Step [535/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [536/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [537/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [538/1407], Loss: 4.6095\n",
      "Epoch [1/2], Step [539/1407], Loss: 4.6093\n",
      "Epoch [1/2], Step [540/1407], Loss: 4.6032\n",
      "Epoch [1/2], Step [541/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [542/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [543/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [544/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [545/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [546/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [547/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [548/1407], Loss: 4.6018\n",
      "Epoch [1/2], Step [549/1407], Loss: 4.6079\n",
      "Epoch [1/2], Step [550/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [551/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [552/1407], Loss: 4.6000\n",
      "Epoch [1/2], Step [553/1407], Loss: 4.6014\n",
      "Epoch [1/2], Step [554/1407], Loss: 4.6110\n",
      "Epoch [1/2], Step [555/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [556/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [557/1407], Loss: 4.6018\n",
      "Epoch [1/2], Step [558/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [559/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [560/1407], Loss: 4.6032\n",
      "Epoch [1/2], Step [561/1407], Loss: 4.6003\n",
      "Epoch [1/2], Step [562/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [563/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [564/1407], Loss: 4.6141\n",
      "Epoch [1/2], Step [565/1407], Loss: 4.5992\n",
      "Epoch [1/2], Step [566/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [567/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [568/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [569/1407], Loss: 4.6009\n",
      "Epoch [1/2], Step [570/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [571/1407], Loss: 4.5950\n",
      "Epoch [1/2], Step [572/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [573/1407], Loss: 4.5980\n",
      "Epoch [1/2], Step [574/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [575/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [576/1407], Loss: 4.6007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [577/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [578/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [579/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [580/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [581/1407], Loss: 4.6096\n",
      "Epoch [1/2], Step [582/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [583/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [584/1407], Loss: 4.6121\n",
      "Epoch [1/2], Step [585/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [586/1407], Loss: 4.6148\n",
      "Epoch [1/2], Step [587/1407], Loss: 4.6124\n",
      "Epoch [1/2], Step [588/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [589/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [590/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [591/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [592/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [593/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [594/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [595/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [596/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [597/1407], Loss: 4.6043\n",
      "Epoch [1/2], Step [598/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [599/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [600/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [601/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [602/1407], Loss: 4.5995\n",
      "Epoch [1/2], Step [603/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [604/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [605/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [606/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [607/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [608/1407], Loss: 4.6118\n",
      "Epoch [1/2], Step [609/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [610/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [611/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [612/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [613/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [614/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [615/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [616/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [617/1407], Loss: 4.6143\n",
      "Epoch [1/2], Step [618/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [619/1407], Loss: 4.6107\n",
      "Epoch [1/2], Step [620/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [621/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [622/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [623/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [624/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [625/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [626/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [627/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [628/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [629/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [630/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [631/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [632/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [633/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [634/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [635/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [636/1407], Loss: 4.5983\n",
      "Epoch [1/2], Step [637/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [638/1407], Loss: 4.6146\n",
      "Epoch [1/2], Step [639/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [640/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [641/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [642/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [643/1407], Loss: 4.6032\n",
      "Epoch [1/2], Step [644/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [645/1407], Loss: 4.5987\n",
      "Epoch [1/2], Step [646/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [647/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [648/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [649/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [650/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [651/1407], Loss: 4.6032\n",
      "Epoch [1/2], Step [652/1407], Loss: 4.6137\n",
      "Epoch [1/2], Step [653/1407], Loss: 4.6000\n",
      "Epoch [1/2], Step [654/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [655/1407], Loss: 4.6109\n",
      "Epoch [1/2], Step [656/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [657/1407], Loss: 4.6007\n",
      "Epoch [1/2], Step [658/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [659/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [660/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [661/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [662/1407], Loss: 4.6131\n",
      "Epoch [1/2], Step [663/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [664/1407], Loss: 4.6116\n",
      "Epoch [1/2], Step [665/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [666/1407], Loss: 4.6142\n",
      "Epoch [1/2], Step [667/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [668/1407], Loss: 4.5943\n",
      "Epoch [1/2], Step [669/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [670/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [671/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [672/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [673/1407], Loss: 4.6145\n",
      "Epoch [1/2], Step [674/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [675/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [676/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [677/1407], Loss: 4.6124\n",
      "Epoch [1/2], Step [678/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [679/1407], Loss: 4.6007\n",
      "Epoch [1/2], Step [680/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [681/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [682/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [683/1407], Loss: 4.5992\n",
      "Epoch [1/2], Step [684/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [685/1407], Loss: 4.5964\n",
      "Epoch [1/2], Step [686/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [687/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [688/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [689/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [690/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [691/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [692/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [693/1407], Loss: 4.6134\n",
      "Epoch [1/2], Step [694/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [695/1407], Loss: 4.5992\n",
      "Epoch [1/2], Step [696/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [697/1407], Loss: 4.6001\n",
      "Epoch [1/2], Step [698/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [699/1407], Loss: 4.6145\n",
      "Epoch [1/2], Step [700/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [701/1407], Loss: 4.6110\n",
      "Epoch [1/2], Step [702/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [703/1407], Loss: 4.6129\n",
      "Epoch [1/2], Step [704/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [705/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [706/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [707/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [708/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [709/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [710/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [711/1407], Loss: 4.6079\n",
      "Epoch [1/2], Step [712/1407], Loss: 4.6002\n",
      "Epoch [1/2], Step [713/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [714/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [715/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [716/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [717/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [718/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [719/1407], Loss: 4.6021\n",
      "Epoch [1/2], Step [720/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [721/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [722/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [723/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [724/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [725/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [726/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [727/1407], Loss: 4.6002\n",
      "Epoch [1/2], Step [728/1407], Loss: 4.6132\n",
      "Epoch [1/2], Step [729/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [730/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [731/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [732/1407], Loss: 4.6129\n",
      "Epoch [1/2], Step [733/1407], Loss: 4.6107\n",
      "Epoch [1/2], Step [734/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [735/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [736/1407], Loss: 4.6100\n",
      "Epoch [1/2], Step [737/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [738/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [739/1407], Loss: 4.6123\n",
      "Epoch [1/2], Step [740/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [741/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [742/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [743/1407], Loss: 4.6119\n",
      "Epoch [1/2], Step [744/1407], Loss: 4.6096\n",
      "Epoch [1/2], Step [745/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [746/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [747/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [748/1407], Loss: 4.6138\n",
      "Epoch [1/2], Step [749/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [750/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [751/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [752/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [753/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [754/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [755/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [756/1407], Loss: 4.6145\n",
      "Epoch [1/2], Step [757/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [758/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [759/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [760/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [761/1407], Loss: 4.5982\n",
      "Epoch [1/2], Step [762/1407], Loss: 4.6112\n",
      "Epoch [1/2], Step [763/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [764/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [765/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [766/1407], Loss: 4.5988\n",
      "Epoch [1/2], Step [767/1407], Loss: 4.6038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [768/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [769/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [770/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [771/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [772/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [773/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [774/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [775/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [776/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [777/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [778/1407], Loss: 4.6137\n",
      "Epoch [1/2], Step [779/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [780/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [781/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [782/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [783/1407], Loss: 4.6150\n",
      "Epoch [1/2], Step [784/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [785/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [786/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [787/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [788/1407], Loss: 4.6121\n",
      "Epoch [1/2], Step [789/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [790/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [791/1407], Loss: 4.6121\n",
      "Epoch [1/2], Step [792/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [793/1407], Loss: 4.6020\n",
      "Epoch [1/2], Step [794/1407], Loss: 4.6113\n",
      "Epoch [1/2], Step [795/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [796/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [797/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [798/1407], Loss: 4.6142\n",
      "Epoch [1/2], Step [799/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [800/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [801/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [802/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [803/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [804/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [805/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [806/1407], Loss: 4.6012\n",
      "Epoch [1/2], Step [807/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [808/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [809/1407], Loss: 4.5953\n",
      "Epoch [1/2], Step [810/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [811/1407], Loss: 4.6034\n",
      "Epoch [1/2], Step [812/1407], Loss: 4.6111\n",
      "Epoch [1/2], Step [813/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [814/1407], Loss: 4.6144\n",
      "Epoch [1/2], Step [815/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [816/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [817/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [818/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [819/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [820/1407], Loss: 4.6134\n",
      "Epoch [1/2], Step [821/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [822/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [823/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [824/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [825/1407], Loss: 4.6024\n",
      "Epoch [1/2], Step [826/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [827/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [828/1407], Loss: 4.6116\n",
      "Epoch [1/2], Step [829/1407], Loss: 4.6001\n",
      "Epoch [1/2], Step [830/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [831/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [832/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [833/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [834/1407], Loss: 4.6102\n",
      "Epoch [1/2], Step [835/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [836/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [837/1407], Loss: 4.6148\n",
      "Epoch [1/2], Step [838/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [839/1407], Loss: 4.6079\n",
      "Epoch [1/2], Step [840/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [841/1407], Loss: 4.6019\n",
      "Epoch [1/2], Step [842/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [843/1407], Loss: 4.6110\n",
      "Epoch [1/2], Step [844/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [845/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [846/1407], Loss: 4.5998\n",
      "Epoch [1/2], Step [847/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [848/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [849/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [850/1407], Loss: 4.6014\n",
      "Epoch [1/2], Step [851/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [852/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [853/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [854/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [855/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [856/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [857/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [858/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [859/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [860/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [861/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [862/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [863/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [864/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [865/1407], Loss: 4.6093\n",
      "Epoch [1/2], Step [866/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [867/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [868/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [869/1407], Loss: 4.6112\n",
      "Epoch [1/2], Step [870/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [871/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [872/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [873/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [874/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [875/1407], Loss: 4.6079\n",
      "Epoch [1/2], Step [876/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [877/1407], Loss: 4.6107\n",
      "Epoch [1/2], Step [878/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [879/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [880/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [881/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [882/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [883/1407], Loss: 4.6106\n",
      "Epoch [1/2], Step [884/1407], Loss: 4.6079\n",
      "Epoch [1/2], Step [885/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [886/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [887/1407], Loss: 4.6120\n",
      "Epoch [1/2], Step [888/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [889/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [890/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [891/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [892/1407], Loss: 4.6096\n",
      "Epoch [1/2], Step [893/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [894/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [895/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [896/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [897/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [898/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [899/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [900/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [901/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [902/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [903/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [904/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [905/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [906/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [907/1407], Loss: 4.6111\n",
      "Epoch [1/2], Step [908/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [909/1407], Loss: 4.6020\n",
      "Epoch [1/2], Step [910/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [911/1407], Loss: 4.6068\n",
      "Epoch [1/2], Step [912/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [913/1407], Loss: 4.6111\n",
      "Epoch [1/2], Step [914/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [915/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [916/1407], Loss: 4.6005\n",
      "Epoch [1/2], Step [917/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [918/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [919/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [920/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [921/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [922/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [923/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [924/1407], Loss: 4.6001\n",
      "Epoch [1/2], Step [925/1407], Loss: 4.5996\n",
      "Epoch [1/2], Step [926/1407], Loss: 4.6004\n",
      "Epoch [1/2], Step [927/1407], Loss: 4.6110\n",
      "Epoch [1/2], Step [928/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [929/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [930/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [931/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [932/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [933/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [934/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [935/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [936/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [937/1407], Loss: 4.6043\n",
      "Epoch [1/2], Step [938/1407], Loss: 4.6012\n",
      "Epoch [1/2], Step [939/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [940/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [941/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [942/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [943/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [944/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [945/1407], Loss: 4.6113\n",
      "Epoch [1/2], Step [946/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [947/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [948/1407], Loss: 4.6130\n",
      "Epoch [1/2], Step [949/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [950/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [951/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [952/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [953/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [954/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [955/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [956/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [957/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [958/1407], Loss: 4.6068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [959/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [960/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [961/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [962/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [963/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [964/1407], Loss: 4.5997\n",
      "Epoch [1/2], Step [965/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [966/1407], Loss: 4.6029\n",
      "Epoch [1/2], Step [967/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [968/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [969/1407], Loss: 4.5990\n",
      "Epoch [1/2], Step [970/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [971/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [972/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [973/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [974/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [975/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [976/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [977/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [978/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [979/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [980/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [981/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [982/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [983/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [984/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [985/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [986/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [987/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [988/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [989/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [990/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [991/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [992/1407], Loss: 4.6000\n",
      "Epoch [1/2], Step [993/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [994/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [995/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [996/1407], Loss: 4.6003\n",
      "Epoch [1/2], Step [997/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [998/1407], Loss: 4.6107\n",
      "Epoch [1/2], Step [999/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [1000/1407], Loss: 4.6010\n",
      "Epoch [1/2], Step [1001/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [1002/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [1003/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [1004/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [1005/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [1006/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1007/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1008/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [1009/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [1010/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [1011/1407], Loss: 4.6100\n",
      "Epoch [1/2], Step [1012/1407], Loss: 4.5986\n",
      "Epoch [1/2], Step [1013/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [1014/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1015/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [1016/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [1017/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [1018/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [1019/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [1020/1407], Loss: 4.6003\n",
      "Epoch [1/2], Step [1021/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [1022/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [1023/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1024/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1025/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [1026/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [1027/1407], Loss: 4.6110\n",
      "Epoch [1/2], Step [1028/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [1029/1407], Loss: 4.6093\n",
      "Epoch [1/2], Step [1030/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [1031/1407], Loss: 4.5994\n",
      "Epoch [1/2], Step [1032/1407], Loss: 4.6105\n",
      "Epoch [1/2], Step [1033/1407], Loss: 4.6118\n",
      "Epoch [1/2], Step [1034/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1035/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [1036/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [1037/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [1038/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [1039/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [1040/1407], Loss: 4.6020\n",
      "Epoch [1/2], Step [1041/1407], Loss: 4.5968\n",
      "Epoch [1/2], Step [1042/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [1043/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [1044/1407], Loss: 4.6127\n",
      "Epoch [1/2], Step [1045/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [1046/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [1047/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1048/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [1049/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [1050/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [1051/1407], Loss: 4.6003\n",
      "Epoch [1/2], Step [1052/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [1053/1407], Loss: 4.6100\n",
      "Epoch [1/2], Step [1054/1407], Loss: 4.6141\n",
      "Epoch [1/2], Step [1055/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1056/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1057/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [1058/1407], Loss: 4.6002\n",
      "Epoch [1/2], Step [1059/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [1060/1407], Loss: 4.6072\n",
      "Epoch [1/2], Step [1061/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [1062/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [1063/1407], Loss: 4.6112\n",
      "Epoch [1/2], Step [1064/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [1065/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [1066/1407], Loss: 4.5973\n",
      "Epoch [1/2], Step [1067/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1068/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [1069/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [1070/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1071/1407], Loss: 4.5998\n",
      "Epoch [1/2], Step [1072/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [1073/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [1074/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [1075/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1076/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [1077/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [1078/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [1079/1407], Loss: 4.6042\n",
      "Epoch [1/2], Step [1080/1407], Loss: 4.6133\n",
      "Epoch [1/2], Step [1081/1407], Loss: 4.6122\n",
      "Epoch [1/2], Step [1082/1407], Loss: 4.6108\n",
      "Epoch [1/2], Step [1083/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [1084/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [1085/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [1086/1407], Loss: 4.6134\n",
      "Epoch [1/2], Step [1087/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [1088/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1089/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1090/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [1091/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [1092/1407], Loss: 4.6018\n",
      "Epoch [1/2], Step [1093/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1094/1407], Loss: 4.6093\n",
      "Epoch [1/2], Step [1095/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [1096/1407], Loss: 4.6006\n",
      "Epoch [1/2], Step [1097/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [1098/1407], Loss: 4.6095\n",
      "Epoch [1/2], Step [1099/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [1100/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [1101/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1102/1407], Loss: 4.6015\n",
      "Epoch [1/2], Step [1103/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [1104/1407], Loss: 4.6144\n",
      "Epoch [1/2], Step [1105/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [1106/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1107/1407], Loss: 4.6122\n",
      "Epoch [1/2], Step [1108/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [1109/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1110/1407], Loss: 4.6006\n",
      "Epoch [1/2], Step [1111/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [1112/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1113/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [1114/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [1115/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [1116/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [1117/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1118/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [1119/1407], Loss: 4.5997\n",
      "Epoch [1/2], Step [1120/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [1121/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [1122/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1123/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [1124/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [1125/1407], Loss: 4.6105\n",
      "Epoch [1/2], Step [1126/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [1127/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [1128/1407], Loss: 4.6022\n",
      "Epoch [1/2], Step [1129/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [1130/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1131/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1132/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1133/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [1134/1407], Loss: 4.5980\n",
      "Epoch [1/2], Step [1135/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [1136/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [1137/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1138/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1139/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1140/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [1141/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [1142/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1143/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [1144/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [1145/1407], Loss: 4.6019\n",
      "Epoch [1/2], Step [1146/1407], Loss: 4.6076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1147/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [1148/1407], Loss: 4.6102\n",
      "Epoch [1/2], Step [1149/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [1150/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [1151/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [1152/1407], Loss: 4.6113\n",
      "Epoch [1/2], Step [1153/1407], Loss: 4.6111\n",
      "Epoch [1/2], Step [1154/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [1155/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1156/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [1157/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [1158/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [1159/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [1160/1407], Loss: 4.6140\n",
      "Epoch [1/2], Step [1161/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [1162/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1163/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [1164/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1165/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [1166/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [1167/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [1168/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [1169/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [1170/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1171/1407], Loss: 4.6007\n",
      "Epoch [1/2], Step [1172/1407], Loss: 4.6030\n",
      "Epoch [1/2], Step [1173/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1174/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [1175/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [1176/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [1177/1407], Loss: 4.6118\n",
      "Epoch [1/2], Step [1178/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [1179/1407], Loss: 4.6007\n",
      "Epoch [1/2], Step [1180/1407], Loss: 4.5976\n",
      "Epoch [1/2], Step [1181/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [1182/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [1183/1407], Loss: 4.6122\n",
      "Epoch [1/2], Step [1184/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [1185/1407], Loss: 4.5985\n",
      "Epoch [1/2], Step [1186/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [1187/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [1188/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [1189/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1190/1407], Loss: 4.6034\n",
      "Epoch [1/2], Step [1191/1407], Loss: 4.6120\n",
      "Epoch [1/2], Step [1192/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [1193/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [1194/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [1195/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [1196/1407], Loss: 4.6006\n",
      "Epoch [1/2], Step [1197/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [1198/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [1199/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [1200/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [1201/1407], Loss: 4.6133\n",
      "Epoch [1/2], Step [1202/1407], Loss: 4.6008\n",
      "Epoch [1/2], Step [1203/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1204/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [1205/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [1206/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1207/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [1208/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [1209/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [1210/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1211/1407], Loss: 4.6161\n",
      "Epoch [1/2], Step [1212/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [1213/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1214/1407], Loss: 4.6101\n",
      "Epoch [1/2], Step [1215/1407], Loss: 4.6112\n",
      "Epoch [1/2], Step [1216/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [1217/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [1218/1407], Loss: 4.6104\n",
      "Epoch [1/2], Step [1219/1407], Loss: 4.6119\n",
      "Epoch [1/2], Step [1220/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1221/1407], Loss: 4.6044\n",
      "Epoch [1/2], Step [1222/1407], Loss: 4.6018\n",
      "Epoch [1/2], Step [1223/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [1224/1407], Loss: 4.6037\n",
      "Epoch [1/2], Step [1225/1407], Loss: 4.6095\n",
      "Epoch [1/2], Step [1226/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [1227/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1228/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [1229/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [1230/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [1231/1407], Loss: 4.6024\n",
      "Epoch [1/2], Step [1232/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [1233/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [1234/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [1235/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [1236/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [1237/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [1238/1407], Loss: 4.6091\n",
      "Epoch [1/2], Step [1239/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [1240/1407], Loss: 4.6109\n",
      "Epoch [1/2], Step [1241/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [1242/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [1243/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [1244/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [1245/1407], Loss: 4.6116\n",
      "Epoch [1/2], Step [1246/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [1247/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [1248/1407], Loss: 4.6126\n",
      "Epoch [1/2], Step [1249/1407], Loss: 4.6055\n",
      "Epoch [1/2], Step [1250/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [1251/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [1252/1407], Loss: 4.6034\n",
      "Epoch [1/2], Step [1253/1407], Loss: 4.6018\n",
      "Epoch [1/2], Step [1254/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [1255/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [1256/1407], Loss: 4.6128\n",
      "Epoch [1/2], Step [1257/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1258/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [1259/1407], Loss: 4.6027\n",
      "Epoch [1/2], Step [1260/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1261/1407], Loss: 4.6124\n",
      "Epoch [1/2], Step [1262/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [1263/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1264/1407], Loss: 4.6034\n",
      "Epoch [1/2], Step [1265/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [1266/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [1267/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [1268/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [1269/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [1270/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1271/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [1272/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [1273/1407], Loss: 4.6092\n",
      "Epoch [1/2], Step [1274/1407], Loss: 4.6089\n",
      "Epoch [1/2], Step [1275/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [1276/1407], Loss: 4.6035\n",
      "Epoch [1/2], Step [1277/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1278/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [1279/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [1280/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1281/1407], Loss: 4.6017\n",
      "Epoch [1/2], Step [1282/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [1283/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [1284/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [1285/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [1286/1407], Loss: 4.6090\n",
      "Epoch [1/2], Step [1287/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1288/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [1289/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [1290/1407], Loss: 4.6136\n",
      "Epoch [1/2], Step [1291/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1292/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1293/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [1294/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [1295/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1296/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [1297/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [1298/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [1299/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [1300/1407], Loss: 4.6040\n",
      "Epoch [1/2], Step [1301/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [1302/1407], Loss: 4.6054\n",
      "Epoch [1/2], Step [1303/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [1304/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1305/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1306/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [1307/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [1308/1407], Loss: 4.6132\n",
      "Epoch [1/2], Step [1309/1407], Loss: 4.6102\n",
      "Epoch [1/2], Step [1310/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [1311/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [1312/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1313/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [1314/1407], Loss: 4.6081\n",
      "Epoch [1/2], Step [1315/1407], Loss: 4.6120\n",
      "Epoch [1/2], Step [1316/1407], Loss: 4.6087\n",
      "Epoch [1/2], Step [1317/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1318/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1319/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [1320/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1321/1407], Loss: 4.6053\n",
      "Epoch [1/2], Step [1322/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [1323/1407], Loss: 4.6026\n",
      "Epoch [1/2], Step [1324/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [1325/1407], Loss: 4.6062\n",
      "Epoch [1/2], Step [1326/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [1327/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [1328/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [1329/1407], Loss: 4.6088\n",
      "Epoch [1/2], Step [1330/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [1331/1407], Loss: 4.6028\n",
      "Epoch [1/2], Step [1332/1407], Loss: 4.6117\n",
      "Epoch [1/2], Step [1333/1407], Loss: 4.6036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1334/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1335/1407], Loss: 4.6115\n",
      "Epoch [1/2], Step [1336/1407], Loss: 4.6043\n",
      "Epoch [1/2], Step [1337/1407], Loss: 4.6073\n",
      "Epoch [1/2], Step [1338/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [1339/1407], Loss: 4.6113\n",
      "Epoch [1/2], Step [1340/1407], Loss: 4.6024\n",
      "Epoch [1/2], Step [1341/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [1342/1407], Loss: 4.6012\n",
      "Epoch [1/2], Step [1343/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [1344/1407], Loss: 4.6016\n",
      "Epoch [1/2], Step [1345/1407], Loss: 4.6039\n",
      "Epoch [1/2], Step [1346/1407], Loss: 4.6099\n",
      "Epoch [1/2], Step [1347/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [1348/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [1349/1407], Loss: 4.6098\n",
      "Epoch [1/2], Step [1350/1407], Loss: 4.6025\n",
      "Epoch [1/2], Step [1351/1407], Loss: 4.6031\n",
      "Epoch [1/2], Step [1352/1407], Loss: 4.6063\n",
      "Epoch [1/2], Step [1353/1407], Loss: 4.6052\n",
      "Epoch [1/2], Step [1354/1407], Loss: 4.6078\n",
      "Epoch [1/2], Step [1355/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [1356/1407], Loss: 4.6082\n",
      "Epoch [1/2], Step [1357/1407], Loss: 4.6131\n",
      "Epoch [1/2], Step [1358/1407], Loss: 4.6041\n",
      "Epoch [1/2], Step [1359/1407], Loss: 4.6036\n",
      "Epoch [1/2], Step [1360/1407], Loss: 4.5991\n",
      "Epoch [1/2], Step [1361/1407], Loss: 4.6047\n",
      "Epoch [1/2], Step [1362/1407], Loss: 4.6045\n",
      "Epoch [1/2], Step [1363/1407], Loss: 4.6069\n",
      "Epoch [1/2], Step [1364/1407], Loss: 4.6086\n",
      "Epoch [1/2], Step [1365/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [1366/1407], Loss: 4.6049\n",
      "Epoch [1/2], Step [1367/1407], Loss: 4.6083\n",
      "Epoch [1/2], Step [1368/1407], Loss: 4.6097\n",
      "Epoch [1/2], Step [1369/1407], Loss: 4.6065\n",
      "Epoch [1/2], Step [1370/1407], Loss: 4.6105\n",
      "Epoch [1/2], Step [1371/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [1372/1407], Loss: 4.6023\n",
      "Epoch [1/2], Step [1373/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [1374/1407], Loss: 4.6085\n",
      "Epoch [1/2], Step [1375/1407], Loss: 4.6077\n",
      "Epoch [1/2], Step [1376/1407], Loss: 4.6059\n",
      "Epoch [1/2], Step [1377/1407], Loss: 4.6056\n",
      "Epoch [1/2], Step [1378/1407], Loss: 4.6058\n",
      "Epoch [1/2], Step [1379/1407], Loss: 4.6050\n",
      "Epoch [1/2], Step [1380/1407], Loss: 4.6070\n",
      "Epoch [1/2], Step [1381/1407], Loss: 4.6071\n",
      "Epoch [1/2], Step [1382/1407], Loss: 4.6048\n",
      "Epoch [1/2], Step [1383/1407], Loss: 4.6033\n",
      "Epoch [1/2], Step [1384/1407], Loss: 4.6113\n",
      "Epoch [1/2], Step [1385/1407], Loss: 4.6060\n",
      "Epoch [1/2], Step [1386/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [1387/1407], Loss: 4.6103\n",
      "Epoch [1/2], Step [1388/1407], Loss: 4.6038\n",
      "Epoch [1/2], Step [1389/1407], Loss: 4.6075\n",
      "Epoch [1/2], Step [1390/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1391/1407], Loss: 4.6051\n",
      "Epoch [1/2], Step [1392/1407], Loss: 4.6076\n",
      "Epoch [1/2], Step [1393/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [1394/1407], Loss: 4.6107\n",
      "Epoch [1/2], Step [1395/1407], Loss: 4.6067\n",
      "Epoch [1/2], Step [1396/1407], Loss: 4.6066\n",
      "Epoch [1/2], Step [1397/1407], Loss: 4.6084\n",
      "Epoch [1/2], Step [1398/1407], Loss: 4.6074\n",
      "Epoch [1/2], Step [1399/1407], Loss: 4.6080\n",
      "Epoch [1/2], Step [1400/1407], Loss: 4.6043\n",
      "Epoch [1/2], Step [1401/1407], Loss: 4.6046\n",
      "Epoch [1/2], Step [1402/1407], Loss: 4.6064\n",
      "Epoch [1/2], Step [1403/1407], Loss: 4.6094\n",
      "Epoch [1/2], Step [1404/1407], Loss: 4.6061\n",
      "Epoch [1/2], Step [1405/1407], Loss: 4.6095\n",
      "Epoch [1/2], Step [1406/1407], Loss: 4.6057\n",
      "Epoch [1/2], Step [1407/1407], Loss: 4.6003\n",
      "Accuracy of the network on the 5000 validation images: 0.86 %\n",
      "Epoch [2/2], Step [1/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [2/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [3/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [4/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [5/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [6/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [7/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [8/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [9/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [10/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [11/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [12/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [13/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [14/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [15/1407], Loss: 4.6103\n",
      "Epoch [2/2], Step [16/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [17/1407], Loss: 4.6006\n",
      "Epoch [2/2], Step [18/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [19/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [20/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [21/1407], Loss: 4.6016\n",
      "Epoch [2/2], Step [22/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [23/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [24/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [25/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [26/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [27/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [28/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [29/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [30/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [31/1407], Loss: 4.5990\n",
      "Epoch [2/2], Step [32/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [33/1407], Loss: 4.6089\n",
      "Epoch [2/2], Step [34/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [35/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [36/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [37/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [38/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [39/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [40/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [41/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [42/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [43/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [44/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [45/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [46/1407], Loss: 4.6131\n",
      "Epoch [2/2], Step [47/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [48/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [49/1407], Loss: 4.5999\n",
      "Epoch [2/2], Step [50/1407], Loss: 4.5988\n",
      "Epoch [2/2], Step [51/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [52/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [53/1407], Loss: 4.6000\n",
      "Epoch [2/2], Step [54/1407], Loss: 4.6010\n",
      "Epoch [2/2], Step [55/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [56/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [57/1407], Loss: 4.5991\n",
      "Epoch [2/2], Step [58/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [59/1407], Loss: 4.6131\n",
      "Epoch [2/2], Step [60/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [61/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [62/1407], Loss: 4.6009\n",
      "Epoch [2/2], Step [63/1407], Loss: 4.6087\n",
      "Epoch [2/2], Step [64/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [65/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [66/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [67/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [68/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [69/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [70/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [71/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [72/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [73/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [74/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [75/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [76/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [77/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [78/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [79/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [80/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [81/1407], Loss: 4.6010\n",
      "Epoch [2/2], Step [82/1407], Loss: 4.6019\n",
      "Epoch [2/2], Step [83/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [84/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [85/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [86/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [87/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [88/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [89/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [90/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [91/1407], Loss: 4.6087\n",
      "Epoch [2/2], Step [92/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [93/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [94/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [95/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [96/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [97/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [98/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [99/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [100/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [101/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [102/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [103/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [104/1407], Loss: 4.6115\n",
      "Epoch [2/2], Step [105/1407], Loss: 4.6137\n",
      "Epoch [2/2], Step [106/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [107/1407], Loss: 4.5990\n",
      "Epoch [2/2], Step [108/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [109/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [110/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [111/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [112/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [113/1407], Loss: 4.5984\n",
      "Epoch [2/2], Step [114/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [115/1407], Loss: 4.6006\n",
      "Epoch [2/2], Step [116/1407], Loss: 4.6062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [117/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [118/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [119/1407], Loss: 4.6025\n",
      "Epoch [2/2], Step [120/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [121/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [122/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [123/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [124/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [125/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [126/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [127/1407], Loss: 4.6102\n",
      "Epoch [2/2], Step [128/1407], Loss: 4.6008\n",
      "Epoch [2/2], Step [129/1407], Loss: 4.6007\n",
      "Epoch [2/2], Step [130/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [131/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [132/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [133/1407], Loss: 4.6117\n",
      "Epoch [2/2], Step [134/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [135/1407], Loss: 4.5997\n",
      "Epoch [2/2], Step [136/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [137/1407], Loss: 4.6004\n",
      "Epoch [2/2], Step [138/1407], Loss: 4.5993\n",
      "Epoch [2/2], Step [139/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [140/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [141/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [142/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [143/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [144/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [145/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [146/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [147/1407], Loss: 4.6028\n",
      "Epoch [2/2], Step [148/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [149/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [150/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [151/1407], Loss: 4.5997\n",
      "Epoch [2/2], Step [152/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [153/1407], Loss: 4.6017\n",
      "Epoch [2/2], Step [154/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [155/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [156/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [157/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [158/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [159/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [160/1407], Loss: 4.6109\n",
      "Epoch [2/2], Step [161/1407], Loss: 4.6014\n",
      "Epoch [2/2], Step [162/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [163/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [164/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [165/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [166/1407], Loss: 4.6018\n",
      "Epoch [2/2], Step [167/1407], Loss: 4.5991\n",
      "Epoch [2/2], Step [168/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [169/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [170/1407], Loss: 4.6000\n",
      "Epoch [2/2], Step [171/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [172/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [173/1407], Loss: 4.6006\n",
      "Epoch [2/2], Step [174/1407], Loss: 4.6028\n",
      "Epoch [2/2], Step [175/1407], Loss: 4.6103\n",
      "Epoch [2/2], Step [176/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [177/1407], Loss: 4.5998\n",
      "Epoch [2/2], Step [178/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [179/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [180/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [181/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [182/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [183/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [184/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [185/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [186/1407], Loss: 4.6028\n",
      "Epoch [2/2], Step [187/1407], Loss: 4.6000\n",
      "Epoch [2/2], Step [188/1407], Loss: 4.5984\n",
      "Epoch [2/2], Step [189/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [190/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [191/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [192/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [193/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [194/1407], Loss: 4.6126\n",
      "Epoch [2/2], Step [195/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [196/1407], Loss: 4.6122\n",
      "Epoch [2/2], Step [197/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [198/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [199/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [200/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [201/1407], Loss: 4.6135\n",
      "Epoch [2/2], Step [202/1407], Loss: 4.6102\n",
      "Epoch [2/2], Step [203/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [204/1407], Loss: 4.6121\n",
      "Epoch [2/2], Step [205/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [206/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [207/1407], Loss: 4.6004\n",
      "Epoch [2/2], Step [208/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [209/1407], Loss: 4.6004\n",
      "Epoch [2/2], Step [210/1407], Loss: 4.6087\n",
      "Epoch [2/2], Step [211/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [212/1407], Loss: 4.6008\n",
      "Epoch [2/2], Step [213/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [214/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [215/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [216/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [217/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [218/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [219/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [220/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [221/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [222/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [223/1407], Loss: 4.6127\n",
      "Epoch [2/2], Step [224/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [225/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [226/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [227/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [228/1407], Loss: 4.6027\n",
      "Epoch [2/2], Step [229/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [230/1407], Loss: 4.6102\n",
      "Epoch [2/2], Step [231/1407], Loss: 4.6103\n",
      "Epoch [2/2], Step [232/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [233/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [234/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [235/1407], Loss: 4.5955\n",
      "Epoch [2/2], Step [236/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [237/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [238/1407], Loss: 4.6110\n",
      "Epoch [2/2], Step [239/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [240/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [241/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [242/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [243/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [244/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [245/1407], Loss: 4.6019\n",
      "Epoch [2/2], Step [246/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [247/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [248/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [249/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [250/1407], Loss: 4.6121\n",
      "Epoch [2/2], Step [251/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [252/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [253/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [254/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [255/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [256/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [257/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [258/1407], Loss: 4.6140\n",
      "Epoch [2/2], Step [259/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [260/1407], Loss: 4.6113\n",
      "Epoch [2/2], Step [261/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [262/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [263/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [264/1407], Loss: 4.6104\n",
      "Epoch [2/2], Step [265/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [266/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [267/1407], Loss: 4.6146\n",
      "Epoch [2/2], Step [268/1407], Loss: 4.6010\n",
      "Epoch [2/2], Step [269/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [270/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [271/1407], Loss: 4.5989\n",
      "Epoch [2/2], Step [272/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [273/1407], Loss: 4.6018\n",
      "Epoch [2/2], Step [274/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [275/1407], Loss: 4.6100\n",
      "Epoch [2/2], Step [276/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [277/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [278/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [279/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [280/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [281/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [282/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [283/1407], Loss: 4.6111\n",
      "Epoch [2/2], Step [284/1407], Loss: 4.6113\n",
      "Epoch [2/2], Step [285/1407], Loss: 4.6087\n",
      "Epoch [2/2], Step [286/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [287/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [288/1407], Loss: 4.6125\n",
      "Epoch [2/2], Step [289/1407], Loss: 4.6006\n",
      "Epoch [2/2], Step [290/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [291/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [292/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [293/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [294/1407], Loss: 4.5988\n",
      "Epoch [2/2], Step [295/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [296/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [297/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [298/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [299/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [300/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [301/1407], Loss: 4.5964\n",
      "Epoch [2/2], Step [302/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [303/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [304/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [305/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [306/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [307/1407], Loss: 4.6095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [308/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [309/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [310/1407], Loss: 4.6129\n",
      "Epoch [2/2], Step [311/1407], Loss: 4.6134\n",
      "Epoch [2/2], Step [312/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [313/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [314/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [315/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [316/1407], Loss: 4.6109\n",
      "Epoch [2/2], Step [317/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [318/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [319/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [320/1407], Loss: 4.6130\n",
      "Epoch [2/2], Step [321/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [322/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [323/1407], Loss: 4.6134\n",
      "Epoch [2/2], Step [324/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [325/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [326/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [327/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [328/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [329/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [330/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [331/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [332/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [333/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [334/1407], Loss: 4.6118\n",
      "Epoch [2/2], Step [335/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [336/1407], Loss: 4.6100\n",
      "Epoch [2/2], Step [337/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [338/1407], Loss: 4.6107\n",
      "Epoch [2/2], Step [339/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [340/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [341/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [342/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [343/1407], Loss: 4.6015\n",
      "Epoch [2/2], Step [344/1407], Loss: 4.6122\n",
      "Epoch [2/2], Step [345/1407], Loss: 4.6014\n",
      "Epoch [2/2], Step [346/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [347/1407], Loss: 4.6114\n",
      "Epoch [2/2], Step [348/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [349/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [350/1407], Loss: 4.6016\n",
      "Epoch [2/2], Step [351/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [352/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [353/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [354/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [355/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [356/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [357/1407], Loss: 4.6101\n",
      "Epoch [2/2], Step [358/1407], Loss: 4.6001\n",
      "Epoch [2/2], Step [359/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [360/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [361/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [362/1407], Loss: 4.5996\n",
      "Epoch [2/2], Step [363/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [364/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [365/1407], Loss: 4.5966\n",
      "Epoch [2/2], Step [366/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [367/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [368/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [369/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [370/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [371/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [372/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [373/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [374/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [375/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [376/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [377/1407], Loss: 4.5974\n",
      "Epoch [2/2], Step [378/1407], Loss: 4.6017\n",
      "Epoch [2/2], Step [379/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [380/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [381/1407], Loss: 4.5987\n",
      "Epoch [2/2], Step [382/1407], Loss: 4.6015\n",
      "Epoch [2/2], Step [383/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [384/1407], Loss: 4.5984\n",
      "Epoch [2/2], Step [385/1407], Loss: 4.6028\n",
      "Epoch [2/2], Step [386/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [387/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [388/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [389/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [390/1407], Loss: 4.6005\n",
      "Epoch [2/2], Step [391/1407], Loss: 4.6128\n",
      "Epoch [2/2], Step [392/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [393/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [394/1407], Loss: 4.5989\n",
      "Epoch [2/2], Step [395/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [396/1407], Loss: 4.6120\n",
      "Epoch [2/2], Step [397/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [398/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [399/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [400/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [401/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [402/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [403/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [404/1407], Loss: 4.6119\n",
      "Epoch [2/2], Step [405/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [406/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [407/1407], Loss: 4.5996\n",
      "Epoch [2/2], Step [408/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [409/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [410/1407], Loss: 4.6024\n",
      "Epoch [2/2], Step [411/1407], Loss: 4.6018\n",
      "Epoch [2/2], Step [412/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [413/1407], Loss: 4.6127\n",
      "Epoch [2/2], Step [414/1407], Loss: 4.6110\n",
      "Epoch [2/2], Step [415/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [416/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [417/1407], Loss: 4.6114\n",
      "Epoch [2/2], Step [418/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [419/1407], Loss: 4.6019\n",
      "Epoch [2/2], Step [420/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [421/1407], Loss: 4.5990\n",
      "Epoch [2/2], Step [422/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [423/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [424/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [425/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [426/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [427/1407], Loss: 4.6149\n",
      "Epoch [2/2], Step [428/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [429/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [430/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [431/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [432/1407], Loss: 4.6027\n",
      "Epoch [2/2], Step [433/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [434/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [435/1407], Loss: 4.5995\n",
      "Epoch [2/2], Step [436/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [437/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [438/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [439/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [440/1407], Loss: 4.6101\n",
      "Epoch [2/2], Step [441/1407], Loss: 4.6126\n",
      "Epoch [2/2], Step [442/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [443/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [444/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [445/1407], Loss: 4.6154\n",
      "Epoch [2/2], Step [446/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [447/1407], Loss: 4.6138\n",
      "Epoch [2/2], Step [448/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [449/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [450/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [451/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [452/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [453/1407], Loss: 4.5967\n",
      "Epoch [2/2], Step [454/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [455/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [456/1407], Loss: 4.5988\n",
      "Epoch [2/2], Step [457/1407], Loss: 4.6008\n",
      "Epoch [2/2], Step [458/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [459/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [460/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [461/1407], Loss: 4.6117\n",
      "Epoch [2/2], Step [462/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [463/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [464/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [465/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [466/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [467/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [468/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [469/1407], Loss: 4.5953\n",
      "Epoch [2/2], Step [470/1407], Loss: 4.6134\n",
      "Epoch [2/2], Step [471/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [472/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [473/1407], Loss: 4.6027\n",
      "Epoch [2/2], Step [474/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [475/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [476/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [477/1407], Loss: 4.5999\n",
      "Epoch [2/2], Step [478/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [479/1407], Loss: 4.6024\n",
      "Epoch [2/2], Step [480/1407], Loss: 4.5996\n",
      "Epoch [2/2], Step [481/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [482/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [483/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [484/1407], Loss: 4.6141\n",
      "Epoch [2/2], Step [485/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [486/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [487/1407], Loss: 4.5991\n",
      "Epoch [2/2], Step [488/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [489/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [490/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [491/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [492/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [493/1407], Loss: 4.5994\n",
      "Epoch [2/2], Step [494/1407], Loss: 4.5974\n",
      "Epoch [2/2], Step [495/1407], Loss: 4.6134\n",
      "Epoch [2/2], Step [496/1407], Loss: 4.6124\n",
      "Epoch [2/2], Step [497/1407], Loss: 4.6123\n",
      "Epoch [2/2], Step [498/1407], Loss: 4.6021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [499/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [500/1407], Loss: 4.5973\n",
      "Epoch [2/2], Step [501/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [502/1407], Loss: 4.6129\n",
      "Epoch [2/2], Step [503/1407], Loss: 4.5999\n",
      "Epoch [2/2], Step [504/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [505/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [506/1407], Loss: 4.5997\n",
      "Epoch [2/2], Step [507/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [508/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [509/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [510/1407], Loss: 4.6010\n",
      "Epoch [2/2], Step [511/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [512/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [513/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [514/1407], Loss: 4.5995\n",
      "Epoch [2/2], Step [515/1407], Loss: 4.6142\n",
      "Epoch [2/2], Step [516/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [517/1407], Loss: 4.6024\n",
      "Epoch [2/2], Step [518/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [519/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [520/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [521/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [522/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [523/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [524/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [525/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [526/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [527/1407], Loss: 4.6008\n",
      "Epoch [2/2], Step [528/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [529/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [530/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [531/1407], Loss: 4.6007\n",
      "Epoch [2/2], Step [532/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [533/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [534/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [535/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [536/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [537/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [538/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [539/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [540/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [541/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [542/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [543/1407], Loss: 4.6113\n",
      "Epoch [2/2], Step [544/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [545/1407], Loss: 4.6107\n",
      "Epoch [2/2], Step [546/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [547/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [548/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [549/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [550/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [551/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [552/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [553/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [554/1407], Loss: 4.6143\n",
      "Epoch [2/2], Step [555/1407], Loss: 4.5989\n",
      "Epoch [2/2], Step [556/1407], Loss: 4.6002\n",
      "Epoch [2/2], Step [557/1407], Loss: 4.5961\n",
      "Epoch [2/2], Step [558/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [559/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [560/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [561/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [562/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [563/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [564/1407], Loss: 4.6128\n",
      "Epoch [2/2], Step [565/1407], Loss: 4.6141\n",
      "Epoch [2/2], Step [566/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [567/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [568/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [569/1407], Loss: 4.6110\n",
      "Epoch [2/2], Step [570/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [571/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [572/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [573/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [574/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [575/1407], Loss: 4.6134\n",
      "Epoch [2/2], Step [576/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [577/1407], Loss: 4.6009\n",
      "Epoch [2/2], Step [578/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [579/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [580/1407], Loss: 4.5987\n",
      "Epoch [2/2], Step [581/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [582/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [583/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [584/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [585/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [586/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [587/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [588/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [589/1407], Loss: 4.5958\n",
      "Epoch [2/2], Step [590/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [591/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [592/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [593/1407], Loss: 4.5997\n",
      "Epoch [2/2], Step [594/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [595/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [596/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [597/1407], Loss: 4.5981\n",
      "Epoch [2/2], Step [598/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [599/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [600/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [601/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [602/1407], Loss: 4.6124\n",
      "Epoch [2/2], Step [603/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [604/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [605/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [606/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [607/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [608/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [609/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [610/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [611/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [612/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [613/1407], Loss: 4.6016\n",
      "Epoch [2/2], Step [614/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [615/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [616/1407], Loss: 4.6166\n",
      "Epoch [2/2], Step [617/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [618/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [619/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [620/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [621/1407], Loss: 4.5989\n",
      "Epoch [2/2], Step [622/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [623/1407], Loss: 4.6129\n",
      "Epoch [2/2], Step [624/1407], Loss: 4.5997\n",
      "Epoch [2/2], Step [625/1407], Loss: 4.6006\n",
      "Epoch [2/2], Step [626/1407], Loss: 4.6147\n",
      "Epoch [2/2], Step [627/1407], Loss: 4.5969\n",
      "Epoch [2/2], Step [628/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [629/1407], Loss: 4.6124\n",
      "Epoch [2/2], Step [630/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [631/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [632/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [633/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [634/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [635/1407], Loss: 4.6106\n",
      "Epoch [2/2], Step [636/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [637/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [638/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [639/1407], Loss: 4.6129\n",
      "Epoch [2/2], Step [640/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [641/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [642/1407], Loss: 4.5973\n",
      "Epoch [2/2], Step [643/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [644/1407], Loss: 4.6139\n",
      "Epoch [2/2], Step [645/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [646/1407], Loss: 4.5990\n",
      "Epoch [2/2], Step [647/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [648/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [649/1407], Loss: 4.6087\n",
      "Epoch [2/2], Step [650/1407], Loss: 4.5995\n",
      "Epoch [2/2], Step [651/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [652/1407], Loss: 4.5914\n",
      "Epoch [2/2], Step [653/1407], Loss: 4.6010\n",
      "Epoch [2/2], Step [654/1407], Loss: 4.5979\n",
      "Epoch [2/2], Step [655/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [656/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [657/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [658/1407], Loss: 4.6024\n",
      "Epoch [2/2], Step [659/1407], Loss: 4.5995\n",
      "Epoch [2/2], Step [660/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [661/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [662/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [663/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [664/1407], Loss: 4.6135\n",
      "Epoch [2/2], Step [665/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [666/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [667/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [668/1407], Loss: 4.6027\n",
      "Epoch [2/2], Step [669/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [670/1407], Loss: 4.6100\n",
      "Epoch [2/2], Step [671/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [672/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [673/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [674/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [675/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [676/1407], Loss: 4.6140\n",
      "Epoch [2/2], Step [677/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [678/1407], Loss: 4.6160\n",
      "Epoch [2/2], Step [679/1407], Loss: 4.5997\n",
      "Epoch [2/2], Step [680/1407], Loss: 4.5983\n",
      "Epoch [2/2], Step [681/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [682/1407], Loss: 4.6107\n",
      "Epoch [2/2], Step [683/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [684/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [685/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [686/1407], Loss: 4.6089\n",
      "Epoch [2/2], Step [687/1407], Loss: 4.6015\n",
      "Epoch [2/2], Step [688/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [689/1407], Loss: 4.6105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [690/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [691/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [692/1407], Loss: 4.5998\n",
      "Epoch [2/2], Step [693/1407], Loss: 4.6115\n",
      "Epoch [2/2], Step [694/1407], Loss: 4.6109\n",
      "Epoch [2/2], Step [695/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [696/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [697/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [698/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [699/1407], Loss: 4.6101\n",
      "Epoch [2/2], Step [700/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [701/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [702/1407], Loss: 4.5973\n",
      "Epoch [2/2], Step [703/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [704/1407], Loss: 4.6173\n",
      "Epoch [2/2], Step [705/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [706/1407], Loss: 4.5998\n",
      "Epoch [2/2], Step [707/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [708/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [709/1407], Loss: 4.6025\n",
      "Epoch [2/2], Step [710/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [711/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [712/1407], Loss: 4.6106\n",
      "Epoch [2/2], Step [713/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [714/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [715/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [716/1407], Loss: 4.6145\n",
      "Epoch [2/2], Step [717/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [718/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [719/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [720/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [721/1407], Loss: 4.6155\n",
      "Epoch [2/2], Step [722/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [723/1407], Loss: 4.6155\n",
      "Epoch [2/2], Step [724/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [725/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [726/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [727/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [728/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [729/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [730/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [731/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [732/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [733/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [734/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [735/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [736/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [737/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [738/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [739/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [740/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [741/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [742/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [743/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [744/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [745/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [746/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [747/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [748/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [749/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [750/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [751/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [752/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [753/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [754/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [755/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [756/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [757/1407], Loss: 4.6104\n",
      "Epoch [2/2], Step [758/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [759/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [760/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [761/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [762/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [763/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [764/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [765/1407], Loss: 4.6117\n",
      "Epoch [2/2], Step [766/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [767/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [768/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [769/1407], Loss: 4.6120\n",
      "Epoch [2/2], Step [770/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [771/1407], Loss: 4.5908\n",
      "Epoch [2/2], Step [772/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [773/1407], Loss: 4.6139\n",
      "Epoch [2/2], Step [774/1407], Loss: 4.6133\n",
      "Epoch [2/2], Step [775/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [776/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [777/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [778/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [779/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [780/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [781/1407], Loss: 4.6117\n",
      "Epoch [2/2], Step [782/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [783/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [784/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [785/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [786/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [787/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [788/1407], Loss: 4.6113\n",
      "Epoch [2/2], Step [789/1407], Loss: 4.6144\n",
      "Epoch [2/2], Step [790/1407], Loss: 4.6113\n",
      "Epoch [2/2], Step [791/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [792/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [793/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [794/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [795/1407], Loss: 4.5962\n",
      "Epoch [2/2], Step [796/1407], Loss: 4.5968\n",
      "Epoch [2/2], Step [797/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [798/1407], Loss: 4.6139\n",
      "Epoch [2/2], Step [799/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [800/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [801/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [802/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [803/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [804/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [805/1407], Loss: 4.6121\n",
      "Epoch [2/2], Step [806/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [807/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [808/1407], Loss: 4.6102\n",
      "Epoch [2/2], Step [809/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [810/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [811/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [812/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [813/1407], Loss: 4.6117\n",
      "Epoch [2/2], Step [814/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [815/1407], Loss: 4.6089\n",
      "Epoch [2/2], Step [816/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [817/1407], Loss: 4.6018\n",
      "Epoch [2/2], Step [818/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [819/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [820/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [821/1407], Loss: 4.6102\n",
      "Epoch [2/2], Step [822/1407], Loss: 4.5993\n",
      "Epoch [2/2], Step [823/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [824/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [825/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [826/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [827/1407], Loss: 4.6107\n",
      "Epoch [2/2], Step [828/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [829/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [830/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [831/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [832/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [833/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [834/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [835/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [836/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [837/1407], Loss: 4.6001\n",
      "Epoch [2/2], Step [838/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [839/1407], Loss: 4.5995\n",
      "Epoch [2/2], Step [840/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [841/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [842/1407], Loss: 4.6146\n",
      "Epoch [2/2], Step [843/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [844/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [845/1407], Loss: 4.6131\n",
      "Epoch [2/2], Step [846/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [847/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [848/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [849/1407], Loss: 4.6145\n",
      "Epoch [2/2], Step [850/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [851/1407], Loss: 4.6024\n",
      "Epoch [2/2], Step [852/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [853/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [854/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [855/1407], Loss: 4.5958\n",
      "Epoch [2/2], Step [856/1407], Loss: 4.6087\n",
      "Epoch [2/2], Step [857/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [858/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [859/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [860/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [861/1407], Loss: 4.5991\n",
      "Epoch [2/2], Step [862/1407], Loss: 4.6107\n",
      "Epoch [2/2], Step [863/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [864/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [865/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [866/1407], Loss: 4.5998\n",
      "Epoch [2/2], Step [867/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [868/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [869/1407], Loss: 4.6127\n",
      "Epoch [2/2], Step [870/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [871/1407], Loss: 4.6117\n",
      "Epoch [2/2], Step [872/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [873/1407], Loss: 4.6119\n",
      "Epoch [2/2], Step [874/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [875/1407], Loss: 4.6089\n",
      "Epoch [2/2], Step [876/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [877/1407], Loss: 4.6016\n",
      "Epoch [2/2], Step [878/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [879/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [880/1407], Loss: 4.6115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [881/1407], Loss: 4.6019\n",
      "Epoch [2/2], Step [882/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [883/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [884/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [885/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [886/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [887/1407], Loss: 4.6014\n",
      "Epoch [2/2], Step [888/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [889/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [890/1407], Loss: 4.6002\n",
      "Epoch [2/2], Step [891/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [892/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [893/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [894/1407], Loss: 4.6015\n",
      "Epoch [2/2], Step [895/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [896/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [897/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [898/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [899/1407], Loss: 4.6136\n",
      "Epoch [2/2], Step [900/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [901/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [902/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [903/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [904/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [905/1407], Loss: 4.6018\n",
      "Epoch [2/2], Step [906/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [907/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [908/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [909/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [910/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [911/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [912/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [913/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [914/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [915/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [916/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [917/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [918/1407], Loss: 4.6014\n",
      "Epoch [2/2], Step [919/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [920/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [921/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [922/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [923/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [924/1407], Loss: 4.6018\n",
      "Epoch [2/2], Step [925/1407], Loss: 4.6102\n",
      "Epoch [2/2], Step [926/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [927/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [928/1407], Loss: 4.6028\n",
      "Epoch [2/2], Step [929/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [930/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [931/1407], Loss: 4.6007\n",
      "Epoch [2/2], Step [932/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [933/1407], Loss: 4.6109\n",
      "Epoch [2/2], Step [934/1407], Loss: 4.6146\n",
      "Epoch [2/2], Step [935/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [936/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [937/1407], Loss: 4.6005\n",
      "Epoch [2/2], Step [938/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [939/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [940/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [941/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [942/1407], Loss: 4.6010\n",
      "Epoch [2/2], Step [943/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [944/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [945/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [946/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [947/1407], Loss: 4.6161\n",
      "Epoch [2/2], Step [948/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [949/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [950/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [951/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [952/1407], Loss: 4.6001\n",
      "Epoch [2/2], Step [953/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [954/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [955/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [956/1407], Loss: 4.6006\n",
      "Epoch [2/2], Step [957/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [958/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [959/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [960/1407], Loss: 4.6100\n",
      "Epoch [2/2], Step [961/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [962/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [963/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [964/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [965/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [966/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [967/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [968/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [969/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [970/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [971/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [972/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [973/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [974/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [975/1407], Loss: 4.5992\n",
      "Epoch [2/2], Step [976/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [977/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [978/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [979/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [980/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [981/1407], Loss: 4.6016\n",
      "Epoch [2/2], Step [982/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [983/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [984/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [985/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [986/1407], Loss: 4.6124\n",
      "Epoch [2/2], Step [987/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [988/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [989/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [990/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [991/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [992/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [993/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [994/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [995/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [996/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [997/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [998/1407], Loss: 4.5999\n",
      "Epoch [2/2], Step [999/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [1000/1407], Loss: 4.6009\n",
      "Epoch [2/2], Step [1001/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [1002/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [1003/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [1004/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [1005/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [1006/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [1007/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [1008/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [1009/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [1010/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [1011/1407], Loss: 4.5993\n",
      "Epoch [2/2], Step [1012/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [1013/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [1014/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [1015/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [1016/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [1017/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [1018/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [1019/1407], Loss: 4.6102\n",
      "Epoch [2/2], Step [1020/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [1021/1407], Loss: 4.5971\n",
      "Epoch [2/2], Step [1022/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [1023/1407], Loss: 4.6034\n",
      "Epoch [2/2], Step [1024/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [1025/1407], Loss: 4.5987\n",
      "Epoch [2/2], Step [1026/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [1027/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [1028/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [1029/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [1030/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [1031/1407], Loss: 4.6021\n",
      "Epoch [2/2], Step [1032/1407], Loss: 4.6103\n",
      "Epoch [2/2], Step [1033/1407], Loss: 4.5999\n",
      "Epoch [2/2], Step [1034/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [1035/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [1036/1407], Loss: 4.6007\n",
      "Epoch [2/2], Step [1037/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [1038/1407], Loss: 4.6015\n",
      "Epoch [2/2], Step [1039/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [1040/1407], Loss: 4.6129\n",
      "Epoch [2/2], Step [1041/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [1042/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [1043/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [1044/1407], Loss: 4.5984\n",
      "Epoch [2/2], Step [1045/1407], Loss: 4.6103\n",
      "Epoch [2/2], Step [1046/1407], Loss: 4.6123\n",
      "Epoch [2/2], Step [1047/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [1048/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [1049/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [1050/1407], Loss: 4.6114\n",
      "Epoch [2/2], Step [1051/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1052/1407], Loss: 4.6182\n",
      "Epoch [2/2], Step [1053/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [1054/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [1055/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [1056/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [1057/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [1058/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [1059/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [1060/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [1061/1407], Loss: 4.6132\n",
      "Epoch [2/2], Step [1062/1407], Loss: 4.6089\n",
      "Epoch [2/2], Step [1063/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [1064/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [1065/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [1066/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [1067/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [1068/1407], Loss: 4.6016\n",
      "Epoch [2/2], Step [1069/1407], Loss: 4.6062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [1070/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [1071/1407], Loss: 4.6013\n",
      "Epoch [2/2], Step [1072/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1073/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [1074/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [1075/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [1076/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [1077/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [1078/1407], Loss: 4.6113\n",
      "Epoch [2/2], Step [1079/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [1080/1407], Loss: 4.6118\n",
      "Epoch [2/2], Step [1081/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [1082/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [1083/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [1084/1407], Loss: 4.6100\n",
      "Epoch [2/2], Step [1085/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1086/1407], Loss: 4.6033\n",
      "Epoch [2/2], Step [1087/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [1088/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [1089/1407], Loss: 4.6111\n",
      "Epoch [2/2], Step [1090/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [1091/1407], Loss: 4.6019\n",
      "Epoch [2/2], Step [1092/1407], Loss: 4.6002\n",
      "Epoch [2/2], Step [1093/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [1094/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [1095/1407], Loss: 4.6110\n",
      "Epoch [2/2], Step [1096/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [1097/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [1098/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [1099/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [1100/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [1101/1407], Loss: 4.6087\n",
      "Epoch [2/2], Step [1102/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [1103/1407], Loss: 4.5989\n",
      "Epoch [2/2], Step [1104/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [1105/1407], Loss: 4.6103\n",
      "Epoch [2/2], Step [1106/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [1107/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1108/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [1109/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [1110/1407], Loss: 4.5993\n",
      "Epoch [2/2], Step [1111/1407], Loss: 4.6117\n",
      "Epoch [2/2], Step [1112/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [1113/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [1114/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [1115/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [1116/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [1117/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [1118/1407], Loss: 4.5996\n",
      "Epoch [2/2], Step [1119/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [1120/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [1121/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [1122/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [1123/1407], Loss: 4.5986\n",
      "Epoch [2/2], Step [1124/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [1125/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [1126/1407], Loss: 4.6140\n",
      "Epoch [2/2], Step [1127/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [1128/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [1129/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [1130/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [1131/1407], Loss: 4.6104\n",
      "Epoch [2/2], Step [1132/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [1133/1407], Loss: 4.6120\n",
      "Epoch [2/2], Step [1134/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [1135/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [1136/1407], Loss: 4.6106\n",
      "Epoch [2/2], Step [1137/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [1138/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1139/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [1140/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [1141/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [1142/1407], Loss: 4.6109\n",
      "Epoch [2/2], Step [1143/1407], Loss: 4.6014\n",
      "Epoch [2/2], Step [1144/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [1145/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [1146/1407], Loss: 4.6107\n",
      "Epoch [2/2], Step [1147/1407], Loss: 4.6024\n",
      "Epoch [2/2], Step [1148/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [1149/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [1150/1407], Loss: 4.6101\n",
      "Epoch [2/2], Step [1151/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [1152/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [1153/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [1154/1407], Loss: 4.6011\n",
      "Epoch [2/2], Step [1155/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [1156/1407], Loss: 4.6151\n",
      "Epoch [2/2], Step [1157/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [1158/1407], Loss: 4.6019\n",
      "Epoch [2/2], Step [1159/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [1160/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [1161/1407], Loss: 4.6109\n",
      "Epoch [2/2], Step [1162/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1163/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [1164/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [1165/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [1166/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [1167/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1168/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [1169/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [1170/1407], Loss: 4.6015\n",
      "Epoch [2/2], Step [1171/1407], Loss: 4.6096\n",
      "Epoch [2/2], Step [1172/1407], Loss: 4.6009\n",
      "Epoch [2/2], Step [1173/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [1174/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [1175/1407], Loss: 4.6009\n",
      "Epoch [2/2], Step [1176/1407], Loss: 4.6085\n",
      "Epoch [2/2], Step [1177/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [1178/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [1179/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [1180/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [1181/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [1182/1407], Loss: 4.6101\n",
      "Epoch [2/2], Step [1183/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [1184/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [1185/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [1186/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [1187/1407], Loss: 4.6027\n",
      "Epoch [2/2], Step [1188/1407], Loss: 4.6089\n",
      "Epoch [2/2], Step [1189/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [1190/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [1191/1407], Loss: 4.6139\n",
      "Epoch [2/2], Step [1192/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [1193/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [1194/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [1195/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [1196/1407], Loss: 4.6129\n",
      "Epoch [2/2], Step [1197/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [1198/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [1199/1407], Loss: 4.6047\n",
      "Epoch [2/2], Step [1200/1407], Loss: 4.6020\n",
      "Epoch [2/2], Step [1201/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [1202/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [1203/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [1204/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [1205/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [1206/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [1207/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [1208/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [1209/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [1210/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [1211/1407], Loss: 4.6055\n",
      "Epoch [2/2], Step [1212/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [1213/1407], Loss: 4.6004\n",
      "Epoch [2/2], Step [1214/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [1215/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [1216/1407], Loss: 4.6032\n",
      "Epoch [2/2], Step [1217/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [1218/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [1219/1407], Loss: 4.6025\n",
      "Epoch [2/2], Step [1220/1407], Loss: 4.6105\n",
      "Epoch [2/2], Step [1221/1407], Loss: 4.6054\n",
      "Epoch [2/2], Step [1222/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [1223/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [1224/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [1225/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [1226/1407], Loss: 4.5993\n",
      "Epoch [2/2], Step [1227/1407], Loss: 4.6121\n",
      "Epoch [2/2], Step [1228/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [1229/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1230/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [1231/1407], Loss: 4.6119\n",
      "Epoch [2/2], Step [1232/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [1233/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1234/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [1235/1407], Loss: 4.6082\n",
      "Epoch [2/2], Step [1236/1407], Loss: 4.6089\n",
      "Epoch [2/2], Step [1237/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [1238/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [1239/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [1240/1407], Loss: 4.5999\n",
      "Epoch [2/2], Step [1241/1407], Loss: 4.6029\n",
      "Epoch [2/2], Step [1242/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [1243/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [1244/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [1245/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [1246/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [1247/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [1248/1407], Loss: 4.6124\n",
      "Epoch [2/2], Step [1249/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [1250/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [1251/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [1252/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [1253/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1254/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [1255/1407], Loss: 4.6107\n",
      "Epoch [2/2], Step [1256/1407], Loss: 4.6018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [1257/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [1258/1407], Loss: 4.5993\n",
      "Epoch [2/2], Step [1259/1407], Loss: 4.6111\n",
      "Epoch [2/2], Step [1260/1407], Loss: 4.6113\n",
      "Epoch [2/2], Step [1261/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [1262/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [1263/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [1264/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [1265/1407], Loss: 4.6012\n",
      "Epoch [2/2], Step [1266/1407], Loss: 4.6067\n",
      "Epoch [2/2], Step [1267/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [1268/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [1269/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [1270/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [1271/1407], Loss: 4.6111\n",
      "Epoch [2/2], Step [1272/1407], Loss: 4.6036\n",
      "Epoch [2/2], Step [1273/1407], Loss: 4.6090\n",
      "Epoch [2/2], Step [1274/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [1275/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [1276/1407], Loss: 4.6022\n",
      "Epoch [2/2], Step [1277/1407], Loss: 4.6093\n",
      "Epoch [2/2], Step [1278/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [1279/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [1280/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [1281/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [1282/1407], Loss: 4.6039\n",
      "Epoch [2/2], Step [1283/1407], Loss: 4.6134\n",
      "Epoch [2/2], Step [1284/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [1285/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [1286/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [1287/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [1288/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [1289/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [1290/1407], Loss: 4.6063\n",
      "Epoch [2/2], Step [1291/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [1292/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [1293/1407], Loss: 4.6112\n",
      "Epoch [2/2], Step [1294/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [1295/1407], Loss: 4.6058\n",
      "Epoch [2/2], Step [1296/1407], Loss: 4.6079\n",
      "Epoch [2/2], Step [1297/1407], Loss: 4.6010\n",
      "Epoch [2/2], Step [1298/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [1299/1407], Loss: 4.6110\n",
      "Epoch [2/2], Step [1300/1407], Loss: 4.6111\n",
      "Epoch [2/2], Step [1301/1407], Loss: 4.6080\n",
      "Epoch [2/2], Step [1302/1407], Loss: 4.6073\n",
      "Epoch [2/2], Step [1303/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [1304/1407], Loss: 4.6027\n",
      "Epoch [2/2], Step [1305/1407], Loss: 4.6103\n",
      "Epoch [2/2], Step [1306/1407], Loss: 4.6051\n",
      "Epoch [2/2], Step [1307/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [1308/1407], Loss: 4.6072\n",
      "Epoch [2/2], Step [1309/1407], Loss: 4.6108\n",
      "Epoch [2/2], Step [1310/1407], Loss: 4.6086\n",
      "Epoch [2/2], Step [1311/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [1312/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [1313/1407], Loss: 4.6018\n",
      "Epoch [2/2], Step [1314/1407], Loss: 4.6064\n",
      "Epoch [2/2], Step [1315/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [1316/1407], Loss: 4.6044\n",
      "Epoch [2/2], Step [1317/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [1318/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [1319/1407], Loss: 4.6110\n",
      "Epoch [2/2], Step [1320/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [1321/1407], Loss: 4.5995\n",
      "Epoch [2/2], Step [1322/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [1323/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [1324/1407], Loss: 4.6053\n",
      "Epoch [2/2], Step [1325/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [1326/1407], Loss: 4.6030\n",
      "Epoch [2/2], Step [1327/1407], Loss: 4.6070\n",
      "Epoch [2/2], Step [1328/1407], Loss: 4.6111\n",
      "Epoch [2/2], Step [1329/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [1330/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [1331/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [1332/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [1333/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [1334/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [1335/1407], Loss: 4.6023\n",
      "Epoch [2/2], Step [1336/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [1337/1407], Loss: 4.6081\n",
      "Epoch [2/2], Step [1338/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [1339/1407], Loss: 4.6061\n",
      "Epoch [2/2], Step [1340/1407], Loss: 4.6056\n",
      "Epoch [2/2], Step [1341/1407], Loss: 4.6084\n",
      "Epoch [2/2], Step [1342/1407], Loss: 4.6049\n",
      "Epoch [2/2], Step [1343/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [1344/1407], Loss: 4.6075\n",
      "Epoch [2/2], Step [1345/1407], Loss: 4.6069\n",
      "Epoch [2/2], Step [1346/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [1347/1407], Loss: 4.6060\n",
      "Epoch [2/2], Step [1348/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [1349/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [1350/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [1351/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [1352/1407], Loss: 4.6120\n",
      "Epoch [2/2], Step [1353/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [1354/1407], Loss: 4.6074\n",
      "Epoch [2/2], Step [1355/1407], Loss: 4.6052\n",
      "Epoch [2/2], Step [1356/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [1357/1407], Loss: 4.6037\n",
      "Epoch [2/2], Step [1358/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [1359/1407], Loss: 4.6048\n",
      "Epoch [2/2], Step [1360/1407], Loss: 4.6066\n",
      "Epoch [2/2], Step [1361/1407], Loss: 4.6040\n",
      "Epoch [2/2], Step [1362/1407], Loss: 4.6097\n",
      "Epoch [2/2], Step [1363/1407], Loss: 4.6094\n",
      "Epoch [2/2], Step [1364/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [1365/1407], Loss: 4.6077\n",
      "Epoch [2/2], Step [1366/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [1367/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [1368/1407], Loss: 4.6019\n",
      "Epoch [2/2], Step [1369/1407], Loss: 4.6028\n",
      "Epoch [2/2], Step [1370/1407], Loss: 4.6088\n",
      "Epoch [2/2], Step [1371/1407], Loss: 4.6026\n",
      "Epoch [2/2], Step [1372/1407], Loss: 4.6002\n",
      "Epoch [2/2], Step [1373/1407], Loss: 4.6050\n",
      "Epoch [2/2], Step [1374/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [1375/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [1376/1407], Loss: 4.6042\n",
      "Epoch [2/2], Step [1377/1407], Loss: 4.6100\n",
      "Epoch [2/2], Step [1378/1407], Loss: 4.6031\n",
      "Epoch [2/2], Step [1379/1407], Loss: 4.6100\n",
      "Epoch [2/2], Step [1380/1407], Loss: 4.6045\n",
      "Epoch [2/2], Step [1381/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [1382/1407], Loss: 4.6078\n",
      "Epoch [2/2], Step [1383/1407], Loss: 4.6071\n",
      "Epoch [2/2], Step [1384/1407], Loss: 4.6015\n",
      "Epoch [2/2], Step [1385/1407], Loss: 4.6068\n",
      "Epoch [2/2], Step [1386/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [1387/1407], Loss: 4.6059\n",
      "Epoch [2/2], Step [1388/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [1389/1407], Loss: 4.6098\n",
      "Epoch [2/2], Step [1390/1407], Loss: 4.6057\n",
      "Epoch [2/2], Step [1391/1407], Loss: 4.6095\n",
      "Epoch [2/2], Step [1392/1407], Loss: 4.6038\n",
      "Epoch [2/2], Step [1393/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [1394/1407], Loss: 4.6076\n",
      "Epoch [2/2], Step [1395/1407], Loss: 4.6035\n",
      "Epoch [2/2], Step [1396/1407], Loss: 4.6092\n",
      "Epoch [2/2], Step [1397/1407], Loss: 4.6027\n",
      "Epoch [2/2], Step [1398/1407], Loss: 4.6083\n",
      "Epoch [2/2], Step [1399/1407], Loss: 4.6062\n",
      "Epoch [2/2], Step [1400/1407], Loss: 4.6065\n",
      "Epoch [2/2], Step [1401/1407], Loss: 4.6043\n",
      "Epoch [2/2], Step [1402/1407], Loss: 4.6046\n",
      "Epoch [2/2], Step [1403/1407], Loss: 4.6106\n",
      "Epoch [2/2], Step [1404/1407], Loss: 4.6041\n",
      "Epoch [2/2], Step [1405/1407], Loss: 4.6091\n",
      "Epoch [2/2], Step [1406/1407], Loss: 4.6099\n",
      "Epoch [2/2], Step [1407/1407], Loss: 4.6090\n",
      "Accuracy of the network on the 5000 validation images: 0.84 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        \n",
    "    # Validation\n",
    "      # a validation dataset allows us to see model progress along the way while saving our true test data for the end\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1itjIa4peSYk"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JKkcfeOocZYa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 1.0 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test dataset\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNbT3YYoHjfEeTla+pwsK0l",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "directML",
   "language": "python",
   "name": "directml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
