------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Knowledge Distillation using: T=4, soft_target_weight = 0.10, true_target_weight = 0.90 and linear learning rate scheduler
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
KD on all compact BERT models family with 2500 examples and 10 epochs

BERT-tiny
baseline = 81.8%
KD_model = 82.4%

BERT-mini
baseline = 86%
KD_model = 85.7%

BERT-small
baseline = 87.8%
KD_model = 87.6%

BERT-medium
baseline = 89.0%
KD_model =  89.56%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 500 examples

10 epochs
baseline = 60.2%
KD_model = 63.6%

30 epochs
baseline = 63%
KD_model = 66.8%

60 epochs
baseline = 62.8%
KD_model = 66.2%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 1000 examples

10 epochs
baseline = 66.1%
KD_model = 65.2%

30 epochs
baseline = 75.5%
KD_model = 76.3%

60 epochs
baeline = 76.2%
KD_model = 77.7%

100 epochs
baseline = 76.7%
KD_model = 78.9%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 5000 examples

10 epochs
baseline
KD_model = 80.2%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-mini with 1000 examples

10 epochs
baseline = 76.7%
KD_model = 79.2%

30 epochs
baseline = 78.7%
KD_model = 81%

60 epochs
baseline = 78.9%
KD_model = 81.3%


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Knowledge Distillation using: T=2, soft_target_weight = 0.50, true_target_weight = 0.50 and linear learning rate scheduler ***Same as speech recognition training in Hinton paper***
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 500 examples

10 epochs
baseline = 60.2%
KD_model = 63.2%

30 epochs
baseline = 63%
KD_model = 66%


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Knowledge Distillation using: T=8, soft_target_weight = 0.50, true_target_weight = 0.50 and cosine with restarts learning rate scheduler
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 500 examples

10 epochs
baseline = 66.4%
KD_model = 64.8%

30 epochs
baseline = 65.4%
KD_model = 67.6%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-mini with 500 examples

10 epochs
baseline = 71.4%
KD_model = 75.8%

30 epochs
baseline = 75%
KD_model = 77.8%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 1000 examples

10 epochs
baseline = 70%
KD_model = 70.6%

30 epochs
baseline = 73.7%
KD_model = 70.2%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-mini with 1000 examples

10 epochs
baseline = 76.5%
KD_model = 75.7%

30 epochs
baseline = 76.6%
KD_model = 77.5%


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Knowledge Distillation using: T=8, soft_target_weight = 0.10, true_target_weight = 0.90 and linear learning rate scheduler ***Same T as MNIST training in Hinton paper***
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 500 examples

10 epochs
baseline = 60.2%
KD_model = 61.8%

30 epochs
baseline = 63%
KD_model = 66.4%


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Knowledge Distillation using: T=4, soft_target_weight = 0.20, true_target_weight = 0.80 and linear learning rate scheduler
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 2500 examples

10 epochs
baseline = 81.8%
KD_model = 78.2%


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Knowledge Distillation using: T=4, soft_target_weight = 0.10, true_target_weight = 0.90 and cosine with restarts learning rate scheduler
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-tiny with 1000 examples

10 epochs
baseline = 66.9%
KD_model = 68.6%

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BERT-mini with 1000 examples

10 epochs
baseline = 77%
KD_model = 78.2%
